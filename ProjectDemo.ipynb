{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-15T08:25:06.131495Z",
     "iopub.status.busy": "2025-05-15T08:25:06.130820Z",
     "iopub.status.idle": "2025-05-15T08:26:54.131636Z",
     "shell.execute_reply": "2025-05-15T08:26:54.130966Z",
     "shell.execute_reply.started": "2025-05-15T08:25:06.131472Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 08:26:39.747331: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747297599.980936      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747297600.044721      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers scikit-learn seaborn matplotlib nltk tqdm gitpython gdown\n",
    "\n",
    "# Import libraries\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "import os, random, datetime, json, re, hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "# SK-learn & stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix)\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Transformers\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          TrainingArguments, Trainer)\n",
    "\n",
    "# Device & seeds\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if device.type=='cuda':\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Make sure results dirs exist\n",
    "for d in ['data/raw','data/processed','logs','models','results']:\n",
    "    os.makedirs(d, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-15T08:25:06.131495Z",
     "iopub.status.busy": "2025-05-15T08:25:06.130820Z",
     "iopub.status.idle": "2025-05-15T08:26:54.131636Z",
     "shell.execute_reply": "2025-05-15T08:26:54.130966Z",
     "shell.execute_reply.started": "2025-05-15T08:25:06.131472Z"
    },
    "trusted": true
   },
   "source": [
    "# Data Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:38:55.229078Z",
     "iopub.status.busy": "2025-05-15T08:38:55.228302Z",
     "iopub.status.idle": "2025-05-15T08:38:55.326740Z",
     "shell.execute_reply": "2025-05-15T08:38:55.326051Z",
     "shell.execute_reply.started": "2025-05-15T08:38:55.229049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to download and extract datasets\n",
    "def download_datasets():\n",
    "    \"\"\"Download and extract real-world phishing and legitimate email corpora\"\"\"\n",
    "    \n",
    "    # Enron email dataset (legitimate emails)\n",
    "    print(\"Downloading Enron dataset...\")\n",
    "    enron_url = f'https://www.cs.cmu.edu/~./enron/enron_mail_20150507.tar.gz'\n",
    "    !wget -P data/raw {enron_url} -q --show-progress\n",
    "    !tar -xzf data/raw/enron_mail_20150507.tar.gz -C data/raw\n",
    "    \n",
    "    # # Nazario phishing corpus\n",
    "    # print(\"Downloading Nazario phishing corpus...\")\n",
    "    # nazario_url = f\"https://monkey.org/%7Ejose/phishing/phishing-corpus-2023.zip\"\n",
    "    # !wget -P data/raw {nazario_url} -q --show-progress\n",
    "    # !unzip -q data/raw/phishing-corpus-2023.zip -d data/raw/phishing\n",
    "    \n",
    "    # Download PhishTank dataset (URLs and metadata only)\n",
    "    print(\"Downloading PhishTank dataset (for URL analysis only)...\")\n",
    "    phishtank_url = f'https://data.phishtank.com/data/online-valid.csv'\n",
    "    !wget -P data/raw {phishtank_url} -q --show-progress\n",
    "    \n",
    "    # Process PhishTank URLs into template-based emails\n",
    "    print(\"Converting PhishTank URLs to synthetic phishing emails...\")\n",
    "    process_phishtank_to_emails()\n",
    "    \n",
    "    # Log dataset metadata\n",
    "    with open('data/dataset_metadata.json', 'w') as f:\n",
    "        metadata = {\n",
    "            'download_date': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'enron_source': enron_url,\n",
    "            'phishtank_source': phishtank_url\n",
    "        }\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(\"All datasets downloaded successfully!\")\n",
    "\n",
    "def process_phishtank_to_emails(output_dir='data/raw/phishing/phishtank'):\n",
    "    \"\"\"Convert PhishTank URLs into synthetic phishing emails\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Check if PhishTank CSV exists\n",
    "    phishtank_csv = 'data/raw/online-valid.csv'\n",
    "    if not os.path.exists(phishtank_csv):\n",
    "        print(\"PhishTank CSV not found, skipping URL processing\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Load PhishTank data\n",
    "        phishtank_df = pd.read_csv(phishtank_csv)\n",
    "        print(f\"Loaded {len(phishtank_df)} PhishTank URLs\")\n",
    "        \n",
    "        # Define email templates to use with the PhishTank URLs\n",
    "        templates = [\n",
    "            \"Dear Customer,\\n\\nWe have detected suspicious activity on your account. Please verify your identity by clicking on the following link:\\n{url}\\n\\nThis is urgent and requires your immediate attention.\\n\\nThank you,\\nSecurity Team\",\n",
    "            \"URGENT: Your account has been temporarily locked\\n\\nWe've noticed unusual login attempts on your account. To restore access, please verify your identity at:\\n{url}\\n\\nIf you do not verify within 24 hours, your account will be permanently suspended.\\n\\nSecurity Department\",\n",
    "            \"Security Alert: Please verify your information\\n\\nWe need to verify your account information to continue providing our services. Click here to update your details:\\n{url}\\n\\nThis is a mandatory security measure.\\n\\nCustomer Support\",\n",
    "            \"Your payment information needs to be updated\\n\\nWe were unable to process your recent payment. Please update your billing information as soon as possible:\\n{url}\\n\\nYour service will be interrupted if this is not resolved.\\n\\nBilling Department\",   \n",
    "            \"Important notification regarding your account\\n\\nThere is an important message regarding your account that requires your attention. Please review it at:\\n{url}\\n\\nThis is time-sensitive information.\\n\\nAccount Services\"\n",
    "        ]\n",
    "        \n",
    "        num_urls = min(500, len(phishtank_df))\n",
    "        sampled_urls = phishtank_df.sample(num_urls, random_state=42)\n",
    "        \n",
    "        # Create synthetic emails using PhishTank URLs\n",
    "        for i, row in enumerate(sampled_urls.itertuples()):\n",
    "            # Get the phishing URL\n",
    "            phish_url = row.url if hasattr(row, 'url') else row[1]  # Adapt to column name\n",
    "            \n",
    "            template = random.choice(templates)\n",
    "            \n",
    "            email_content = template.format(url=phish_url)\n",
    "            \n",
    "            # Save to file\n",
    "            filename = f\"{output_dir}/phishtank_{i:04d}.txt\"\n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(email_content)\n",
    "                \n",
    "        print(f\"Created {num_urls} synthetic phishing emails from PhishTank URLs\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PhishTank data: {e}\")\n",
    "        print(\"Continuing with other data sources...\")\n",
    "\n",
    "# Function to preprocess emails\n",
    "def preprocess_emails():\n",
    "    \"\"\"Preprocess raw emails: strip headers, normalize URLs, remove duplicates\"\"\"\n",
    "    \n",
    "    def extract_email_body(email_text):\n",
    "        \"\"\"Extract body from email by removing headers\"\"\"\n",
    "        if not email_text:\n",
    "            return \"\"\n",
    "            \n",
    "        lines = email_text.split('\\n')\n",
    "        body_start = 0\n",
    "        \n",
    "        # Find where headers end and body begins\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip() == '':\n",
    "                body_start = i + 1\n",
    "                break\n",
    "        \n",
    "        body = '\\n'.join(lines[body_start:])\n",
    "        return body\n",
    "    \n",
    "    def normalize_urls(text):\n",
    "        \"\"\"Normalize URLs in text to avoid detection based on specific URLs\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "            \n",
    "        # Simple URL normalization - replace actual domains with placeholders\n",
    "        url_pattern = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "        return re.sub(url_pattern, '[URL]', text)\n",
    "    \n",
    "    # Process legitimate emails (Enron)\n",
    "    print(\"Processing legitimate emails...\")\n",
    "    legitimate_emails = []\n",
    "    legitimate_dir = 'data/raw/maildir'\n",
    "    \n",
    "    # Sample a subset of Enron folders to keep dataset size manageable\n",
    "    user_folders = os.listdir(legitimate_dir)\n",
    "    sampled_users = random.sample(user_folders, min(20, len(user_folders)))\n",
    "    \n",
    "    for user in tqdm(sampled_users):\n",
    "        user_path = os.path.join(legitimate_dir, user)\n",
    "        if not os.path.isdir(user_path):\n",
    "            continue\n",
    "            \n",
    "        # Walk through user's email folders\n",
    "        for root, _, files in os.walk(user_path):\n",
    "            for file in files:\n",
    "                if len(legitimate_emails) >= 5000:  # Limit the number of legitimate emails\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    with open(os.path.join(root, file), 'r', encoding='latin1') as f:\n",
    "                        email_text = f.read()\n",
    "                    \n",
    "                    # Extract body and normalize\n",
    "                    body = extract_email_body(email_text)\n",
    "                    body = normalize_urls(body)\n",
    "                    \n",
    "                    if len(body.strip()) > 100:  # Filter out very short emails\n",
    "                        legitimate_emails.append({'text': body, 'label': 0})\n",
    "                except Exception as e:\n",
    "                    continue  # Skip problematic files\n",
    "    \n",
    "    # # Process phishing emails (Nazario)\n",
    "    # print(\"Processing phishing emails...\")\n",
    "    phishing_emails = []\n",
    "    phishing_dir = 'data/raw/phishing'\n",
    "    \n",
    "    # Walk through phishing corpus folders\n",
    "    for root, _, files in os.walk(phishing_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt') or file.endswith('.eml'):\n",
    "                try:\n",
    "                    with open(os.path.join(root, file), 'r', encoding='latin1') as f:\n",
    "                        email_text = f.read()\n",
    "                    \n",
    "                    # Extract body and normalize\n",
    "                    body = extract_email_body(email_text)\n",
    "                    body = normalize_urls(body)\n",
    "                    \n",
    "                    if len(body.strip()) > 100:  # Filter out very short emails\n",
    "                        phishing_emails.append({'text': body, 'label': 1})\n",
    "                except Exception as e:\n",
    "                    continue  # Skip problematic files\n",
    "    \n",
    "    # Remove duplicates\n",
    "    print(\"Removing duplicates...\")\n",
    "    \n",
    "    def get_email_hash(email):\n",
    "        \"\"\"Create a hash of email text to identify duplicates\"\"\"\n",
    "        return hashlib.md5(email['text'].encode()).hexdigest()\n",
    "    \n",
    "    unique_emails = {}\n",
    "    \n",
    "    for email in legitimate_emails + phishing_emails:\n",
    "        email_hash = get_email_hash(email)\n",
    "        if email_hash not in unique_emails:\n",
    "            unique_emails[email_hash] = email\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(list(unique_emails.values()))\n",
    "    \n",
    "    # Balance dataset if needed\n",
    "    min_class_count = min(sum(df['label'] == 0), sum(df['label'] == 1))\n",
    "    legitimate_df = df[df['label'] == 0].sample(min_class_count, random_state=42)\n",
    "    phishing_df = df[df['label'] == 1].sample(min_class_count, random_state=42)\n",
    "    df_balanced = pd.concat([legitimate_df, phishing_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Generate additional synthetic phishing with GPT-2\n",
    "    if len(phishing_df) < 1500:\n",
    "        gpt_samples_needed = 1500 - len(phishing_df)\n",
    "        gpt_phishing = generate_gpt2_phishing(gpt_samples_needed)\n",
    "        gpt_df = pd.DataFrame(gpt_phishing)\n",
    "        df_balanced = pd.concat([df_balanced, gpt_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Split into train/val/test\n",
    "    train_df, temp_df = train_test_split(df_balanced, test_size=0.3, stratify=df_balanced['label'], random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
    "    \n",
    "    # Save processed datasets\n",
    "    train_df.to_csv('data/processed/train.csv', index=False)\n",
    "    val_df.to_csv('data/processed/val.csv', index=False)\n",
    "    test_df.to_csv('data/processed/test.csv', index=False)\n",
    "    \n",
    "    print(f\"Dataset processed and split successfully!\")\n",
    "    print(f\"Train: {len(train_df)} samples\")\n",
    "    print(f\"Validation: {len(val_df)} samples\")\n",
    "    print(f\"Test: {len(test_df)} samples\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Generate synthetic phishing emails with GPT-2\n",
    "def generate_gpt2_phishing(count=1500):\n",
    "    \"\"\"Generate synthetic phishing emails using fine-tuned GPT-2\"\"\"\n",
    "    try:\n",
    "        from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "        \n",
    "        print(f\"Generating {count} synthetic phishing emails with GPT-2...\")\n",
    "        \n",
    "        # Load pre-trained GPT-2 model and tokenizer\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        model.to(device)\n",
    "        \n",
    "        # Set seed for reproducibility\n",
    "        set_seed(42)\n",
    "        \n",
    "        # Phishing email prompts to guide generation\n",
    "        prompts = [\n",
    "            \"Dear Customer, We have detected suspicious activity on your account.\",\n",
    "            \"URGENT: Your account has been temporarily suspended due to\",\n",
    "            \"Security Alert: Your password needs to be reset immediately.\",\n",
    "            \"Attention: Please verify your information to avoid service interruption.\",\n",
    "            \"Important update regarding your account security:\"\n",
    "        ]\n",
    "        \n",
    "        synthetic_phishing = []\n",
    "        \n",
    "        for i in range(count):\n",
    "            prompt = random.choice(prompts)\n",
    "            \n",
    "            # Encode prompt\n",
    "            input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "            \n",
    "            # Generate text\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_length=150,\n",
    "                temperature=0.9,\n",
    "                top_p=0.92,\n",
    "                top_k=50,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            # Decode and clean text\n",
    "            text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Add to collection\n",
    "            synthetic_phishing.append({\n",
    "                \"text\": text,\n",
    "                \"label\": 1,  # 1 indicates phishing\n",
    "                \"synthetic\": True\n",
    "            })\n",
    "            \n",
    "            # Show progress\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Generated {i + 1}/{count} synthetic emails\")\n",
    "        \n",
    "        return synthetic_phishing\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating GPT-2 emails: {e}\")\n",
    "        # Fall back to template generation\n",
    "        template_gen = TemplateGenerator()\n",
    "        return template_gen.generate(count)\n",
    "\n",
    "def download_preprocessed_data(n_samples=1000, download_nltk=True):\n",
    "    \"\"\"Generate synthetic dataset for demonstration\"\"\"\n",
    "    print(\"Creating synthetic dataset for demonstration...\")\n",
    "    \n",
    "    # Download NLTK resources for text generation if needed\n",
    "    if download_nltk:\n",
    "        try:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('wordnet', quiet=True)\n",
    "            try:\n",
    "                nltk.download('omw-1.4', quiet=True)\n",
    "            except:\n",
    "                pass  # Optional resource, ok if it fails\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not download NLTK resources: {e}\")\n",
    "            print(\"Continuing with generation anyway...\")\n",
    "    \n",
    "    # Create synthetic legitimate emails\n",
    "    legitimate_templates = [\n",
    "        \"Dear {name}, I hope this email finds you well. Just wanted to follow up on our meeting last week. Let's schedule a call to discuss the project progress. Best regards, {sender}\",\n",
    "        \"Hello {name}, Please find attached the quarterly report you requested. Let me know if you need anything else. Thanks, {sender}\",\n",
    "        \"Hi team, Reminder about our weekly standup tomorrow at 10AM. Please update your progress on the Jira board. Thanks, {sender}\",\n",
    "        \"Good morning {name}, Just checking in regarding the proposal we submitted last month. Do you have any feedback for us? Best, {sender}\",\n",
    "        \"Dear colleagues, Please note that the office will be closed next Monday for the holiday. All deadlines remain unchanged. Regards, HR\"\n",
    "    ]\n",
    "    \n",
    "    # Create synthetic phishing emails\n",
    "    phishing_templates = [\n",
    "        \"URGENT: Your account has been compromised. Click here to reset your password immediately: {url}\",\n",
    "        \"Dear valued customer, We've noticed suspicious activity on your account. Please verify your identity by clicking this link: {url}\",\n",
    "        \"Congratulations! You've won a $1000 Amazon gift card. Claim now at: {url}\",\n",
    "        \"Your payment was declined. Update your billing information here: {url} to avoid service interruption.\",\n",
    "        \"Security alert: Unusual login detected. If this wasn't you, secure your account immediately: {url}\"\n",
    "    ]\n",
    "    \n",
    "    names = [\"John\", \"Mary\", \"Robert\", \"Patricia\", \"Michael\", \"Jennifer\", \"William\", \"Linda\", \"David\", \"Elizabeth\"]\n",
    "    senders = [\"Mark\", \"Sarah\", \"Alex\", \"Rachel\", \"Tom\", \"Emma\", \"James\", \"Sophie\", \"Daniel\", \"Olivia\"]\n",
    "    urls = [\"http://secure-login.com/verify\", \"http://account-verification.net/secure\", \n",
    "            \"http://amazon-rewards.com/claim\", \"http://banking-update.com/renew\",\n",
    "            \"http://security-check.org/protect\"]\n",
    "    \n",
    "    def generate_emails(templates, count, label, is_phishing=False):\n",
    "        \"\"\"Generate synthetic emails from templates\"\"\"\n",
    "        emails = []\n",
    "        for _ in range(count):\n",
    "            template = random.choice(templates)\n",
    "            if is_phishing:\n",
    "                email = template.format(\n",
    "                    name=random.choice(names), \n",
    "                    sender=random.choice(senders), \n",
    "                    url=random.choice(urls)\n",
    "                )\n",
    "            else:\n",
    "                email = template.format(\n",
    "                    name=random.choice(names), \n",
    "                    sender=random.choice(senders)\n",
    "                )\n",
    "            emails.append({\"text\": email, \"label\": label})\n",
    "        return emails\n",
    "    \n",
    "    # Generate balanced dataset with controlled randomness\n",
    "    set_seed(42) \n",
    "    legitimate_emails = generate_emails(legitimate_templates, n_samples, 0)\n",
    "    phishing_emails = generate_emails(phishing_templates, n_samples, 1, is_phishing=True)\n",
    "    \n",
    "    all_emails = legitimate_emails + phishing_emails\n",
    "    random.shuffle(all_emails)\n",
    "    \n",
    "    df = pd.DataFrame(all_emails)\n",
    "    \n",
    "    # Split into train/val/test with fixed random state\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
    "    \n",
    "    # Save processed datasets\n",
    "    os.makedirs('data/processed', exist_ok=True)\n",
    "    train_df.to_csv('data/processed/train.csv', index=False)\n",
    "    val_df.to_csv('data/processed/val.csv', index=False)\n",
    "    test_df.to_csv('data/processed/test.csv', index=False)\n",
    "    \n",
    "    print(f\"Synthetic dataset created and split successfully!\")\n",
    "    print(f\"Train: {len(train_df)} samples, Legitimate: {sum(train_df['label'] == 0)}, Phishing: {sum(train_df['label'] == 1)}\")\n",
    "    print(f\"Validation: {len(val_df)} samples, Legitimate: {sum(val_df['label'] == 0)}, Phishing: {sum(val_df['label'] == 1)}\")\n",
    "    print(f\"Test: {len(test_df)} samples, Legitimate: {sum(test_df['label'] == 0)}, Phishing: {sum(test_df['label'] == 1)}\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "# Main data loading function\n",
    "def load_real_data():\n",
    "    \"\"\"Load real data or process it if needed\"\"\"\n",
    "    # Check if processed data already exists\n",
    "    if os.path.exists('data/processed/train.csv') and \\\n",
    "       os.path.exists('data/processed/val.csv') and \\\n",
    "       os.path.exists('data/processed/test.csv'):\n",
    "        # Load existing processed data\n",
    "        print(\"Loading existing processed datasets...\")\n",
    "        train_df = pd.read_csv('data/processed/train.csv')\n",
    "        val_df = pd.read_csv('data/processed/val.csv')\n",
    "        test_df = pd.read_csv('data/processed/test.csv')\n",
    "    else:\n",
    "        # Download and process data\n",
    "        print(\"Downloading and processing datasets...\")\n",
    "        try:\n",
    "            if not os.path.exists('data/raw/maildir'):\n",
    "                download_datasets()\n",
    "            train_df, val_df, test_df = preprocess_emails()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing real data: {e}\")\n",
    "            print(\"Falling back to synthetic data...\")\n",
    "            train_df, val_df, test_df = download_preprocessed_data(2000)\n",
    "    \n",
    "    print(f\"Training set: {len(train_df)} emails ({sum(train_df['label'] == 1)} phishing, {sum(train_df['label'] == 0)} legitimate)\")\n",
    "    print(f\"Validation set: {len(val_df)} emails ({sum(val_df['label'] == 1)} phishing, {sum(val_df['label'] == 0)} legitimate)\")\n",
    "    print(f\"Test set: {len(test_df)} emails ({sum(test_df['label'] == 1)} phishing, {sum(test_df['label'] == 0)} legitimate)\")\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },

   "source": [
    "# Template Generator & Perturbation Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:38:58.210733Z",
     "iopub.status.busy": "2025-05-15T08:38:58.210011Z",
     "iopub.status.idle": "2025-05-15T08:38:58.239865Z",
     "shell.execute_reply": "2025-05-15T08:38:58.239202Z",
     "shell.execute_reply.started": "2025-05-15T08:38:58.210706Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TemplateGenerator:\n",
    "    \"\"\"Generate phishing emails from templates\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.templates = [\n",
    "            # Amazon templates\n",
    "            {\n",
    "                \"subject\": \"Amazon: Action Required - Update Payment Information\",\n",
    "                \"body\": \"Dear Amazon Customer,\\n\\nWe need you to update your payment information for your recent purchase. \"\n",
    "                        \"If you do not update within 24 hours, your order will be canceled.\\n\\n\"\n",
    "                        \"Please click here to update: {url}\\n\\n\"\n",
    "                        \"Amazon Customer Service\"\n",
    "            },\n",
    "            {\n",
    "                \"subject\": \"Your Amazon order has shipped\",\n",
    "                \"body\": \"Hello,\\n\\nYour order #{order_num} has shipped and will be delivered on {date}.\\n\\n\"\n",
    "                        \"However, we noticed an issue with your payment method. Please verify your information: {url}\\n\\n\"\n",
    "                        \"Amazon Shipping Team\"\n",
    "            },\n",
    "            \n",
    "            # Banking templates\n",
    "            {\n",
    "                \"subject\": \"Important: Your account access has been limited\",\n",
    "                \"body\": \"Dear {bank} Customer,\\n\\nWe have temporarily limited access to your account due to \"\n",
    "                        \"failed login attempts. To restore full access, please verify your identity: {url}\\n\\n\"\n",
    "                        \"Security Department, {bank}\"\n",
    "            },\n",
    "            {\n",
    "                \"subject\": \"Security Alert: Unusual Activity Detected\",\n",
    "                \"body\": \"Dear {name},\\n\\nWe detected unusual activity on your {bank} account on {date}. \"\n",
    "                        \"If this was not you, please secure your account immediately: {url}\\n\\n\"\n",
    "                        \"Thank you,\\n{bank} Fraud Prevention Team\"\n",
    "            },\n",
    "            \n",
    "            # HR/Corporate templates\n",
    "            {\n",
    "                \"subject\": \"Urgent: Update your company credentials\",\n",
    "                \"body\": \"Dear {name},\\n\\nDue to recent security upgrades, all employees are required to update \"\n",
    "                        \"their login credentials by end of day. Click here to update: {url}\\n\\n\"\n",
    "                        \"IT Department\"\n",
    "            },\n",
    "            {\n",
    "                \"subject\": \"Important: New company policy document\",\n",
    "                \"body\": \"All staff,\\n\\nA new company policy regarding remote work has been published. \"\n",
    "                        \"All employees must read and acknowledge receipt by tomorrow.\\n\\n\"\n",
    "                        \"Download the document here: {url}\\n\\n\"\n",
    "                        \"Human Resources\"\n",
    "            },\n",
    "            \n",
    "            # Tax/Government templates\n",
    "            {\n",
    "                \"subject\": \"IRS: Tax Refund Notification\",\n",
    "                \"body\": \"Tax Refund Notice #{notice_num}\\n\\nDear Taxpayer,\\n\\nAfter the annual calculation of your fiscal activity, \"\n",
    "                        \"we have determined that you are eligible for a refund of ${amount}.\\n\\n\"\n",
    "                        \"Submit your refund request here: {url}\\n\\n\"\n",
    "                        \"Internal Revenue Service\"\n",
    "            },\n",
    "            {\n",
    "                \"subject\": \"Action Required: Government Stimulus Payment\",\n",
    "                \"body\": \"NOTICE: Economic Impact Payment\\n\\nYou have qualified for a government stimulus payment of ${amount}. \"\n",
    "                        \"To receive your payment, please confirm your information: {url}\\n\\n\"\n",
    "                        \"Department of the Treasury\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.names = [\"John Smith\", \"Mary Johnson\", \"Robert Williams\", \"Patricia Brown\", \"Michael Davis\"]\n",
    "        self.banks = [\"Chase\", \"Bank of America\", \"Wells Fargo\", \"Citibank\", \"Capital One\"]\n",
    "        self.dates = [\"May 15, 2023\", \"June 22, 2023\", \"July 8, 2023\", \"August 30, 2023\"]\n",
    "        self.amounts = [\"1,247.63\", \"958.29\", \"2,361.45\", \"785.12\", \"1,503.87\"]\n",
    "        self.order_nums = [\"A23B7C\", \"X92Y14\", \"L67M39\", \"P45Q81\", \"R72S05\"]\n",
    "        self.notice_nums = [\"CP-1234\", \"RF-5678\", \"TX-9012\", \"IR-3456\", \"GV-7890\"]\n",
    "        \n",
    "        # Phishing URLs for templates\n",
    "        self.urls = [\n",
    "            \"http://amazonn-secure.com/verify\",\n",
    "            \"http://security-bankaccess.net/login\",\n",
    "            \"http://company-portal.co/document\",\n",
    "            \"http://tax-refund-secure.com/claim\",\n",
    "            \"http://accountverify-secure.com/auth\"\n",
    "        ]\n",
    "    \n",
    "    def generate(self, n_samples=10, use_template_idx=None):\n",
    "        \"\"\"Generate phishing emails from templates\"\"\"\n",
    "        generated_emails = []\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            if use_template_idx is not None and use_template_idx < len(self.templates):\n",
    "                template = self.templates[use_template_idx]\n",
    "            else:\n",
    "                template = random.choice(self.templates)\n",
    "            \n",
    "            # Fill in template placeholders\n",
    "            body = template[\"body\"].format(\n",
    "                name=random.choice(self.names),\n",
    "                bank=random.choice(self.banks),\n",
    "                date=random.choice(self.dates),\n",
    "                amount=random.choice(self.amounts),\n",
    "                order_num=random.choice(self.order_nums),\n",
    "                notice_num=random.choice(self.notice_nums),\n",
    "                url=random.choice(self.urls)\n",
    "            )\n",
    "            \n",
    "            generated_emails.append({\n",
    "                \"text\": body,\n",
    "                \"label\": 1,  # indicates phishing\n",
    "                \"template_id\": self.templates.index(template)\n",
    "            })\n",
    "        \n",
    "        return generated_emails\n",
    "\n",
    "class PerturbationEngine:\n",
    "    \"\"\"Apply various perturbation techniques to phishing emails to evade detection\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Download NLTK resources if not already available\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        \n",
    "        # Character-level perturbations\n",
    "        self.char_perturbations = [\n",
    "            self._swap_chars,\n",
    "            self._add_char,\n",
    "            self._remove_char,\n",
    "            self._similar_char_replacement\n",
    "        ]\n",
    "        \n",
    "        # Word-level perturbations\n",
    "        self.word_perturbations = [\n",
    "            self._synonym_replacement,\n",
    "            self._word_insertion,\n",
    "            self._word_deletion\n",
    "        ]\n",
    "        \n",
    "        # Style perturbations\n",
    "        self.style_perturbations = [\n",
    "            self._change_case,\n",
    "            self._add_html_formatting,\n",
    "            self._add_unicode_chars\n",
    "        ]\n",
    "        \n",
    "        # URL hiding techniques\n",
    "        self.url_perturbations = [\n",
    "            self._hide_url_in_text,\n",
    "            self._url_shortener_simulation,\n",
    "            self._html_url_obfuscation\n",
    "        ]\n",
    "        \n",
    "        # Similar-looking character mappings\n",
    "        self.char_map = {\n",
    "            'a': ['а', '@', '4'],  # Cyrillic 'а' looks like Latin 'a'\n",
    "            'b': ['b', '6', 'б'],\n",
    "            'c': ['с', '('],  # Cyrillic 'с' looks like Latin 'c'\n",
    "            'e': ['е', '3'],  # Cyrillic 'е' looks like Latin 'e'\n",
    "            'i': ['і', '1', '!'],\n",
    "            'l': ['l', '1', '|'],\n",
    "            'o': ['о', '0'],  # Cyrillic 'о' looks like Latin 'o'\n",
    "            's': ['ѕ', '5', '$'],\n",
    "            't': ['т', '+'],  # Cyrillic 'т' looks like Latin 't'\n",
    "            'w': ['vv', 'ѡ'],\n",
    "            'g': ['g', '9'],\n",
    "            'r': ['r', 'г'],\n",
    "            'n': ['n', 'п'],\n",
    "            'm': ['m', 'м']\n",
    "        }\n",
    "    \n",
    "    def perturb(self, email, technique=None, intensity=0.1):\n",
    "        \"\"\"Apply perturbations to an email\"\"\"\n",
    "        email_text = email[\"text\"]\n",
    "        \n",
    "        # Choose perturbation technique if not specified\n",
    "        if technique is None:\n",
    "            all_techniques = (self.char_perturbations + self.word_perturbations + \n",
    "                             self.style_perturbations + self.url_perturbations)\n",
    "            technique = random.choice(all_techniques)\n",
    "        \n",
    "        # Apply selected perturbation\n",
    "        perturbed_text = technique(email_text, intensity)\n",
    "        \n",
    "        # Create perturbed email\n",
    "        perturbed_email = email.copy()\n",
    "        perturbed_email[\"text\"] = perturbed_text\n",
    "        perturbed_email[\"perturbation\"] = technique.__name__\n",
    "        \n",
    "        return perturbed_email\n",
    "    \n",
    "    def _swap_chars(self, text, intensity):\n",
    "        \"\"\"Swap adjacent characters in words\"\"\"\n",
    "        words = text.split()\n",
    "        new_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            if len(word) <= 1 or random.random() > intensity:\n",
    "                new_words.append(word)\n",
    "                continue\n",
    "                \n",
    "            # Choose a random position for swapping\n",
    "            pos = random.randint(0, len(word) - 2)\n",
    "            chars = list(word)\n",
    "            chars[pos], chars[pos + 1] = chars[pos + 1], chars[pos]\n",
    "            new_words.append(''.join(chars))\n",
    "                \n",
    "        return ' '.join(new_words)\n",
    "    \n",
    "    def _add_char(self, text, intensity):\n",
    "        \"\"\"Add extra characters to words\"\"\"\n",
    "        words = text.split()\n",
    "        new_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            if len(word) == 0 or random.random() > intensity:\n",
    "                new_words.append(word)\n",
    "                continue\n",
    "                \n",
    "            # Insert a random character at a random position\n",
    "            pos = random.randint(0, len(word))\n",
    "            char = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "            new_word = word[:pos] + char + word[pos:]\n",
    "            new_words.append(new_word)\n",
    "                \n",
    "        return ' '.join(new_words)\n",
    "    \n",
    "    def _remove_char(self, text, intensity):\n",
    "        \"\"\"Remove characters from words\"\"\"\n",
    "        words = text.split()\n",
    "        new_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            if len(word) <= 1 or random.random() > intensity:\n",
    "                new_words.append(word)\n",
    "                continue\n",
    "                \n",
    "            # Remove a random character\n",
    "            pos = random.randint(0, len(word) - 1)\n",
    "            new_word = word[:pos] + word[pos + 1:]\n",
    "            new_words.append(new_word)\n",
    "                \n",
    "        return ' '.join(new_words)\n",
    "    \n",
    "    def _similar_char_replacement(self, text, intensity):\n",
    "        \"\"\"Replace characters with similar-looking ones\"\"\"\n",
    "        new_text = \"\"\n",
    "        \n",
    "        for char in text:\n",
    "            lower_char = char.lower()\n",
    "            if lower_char in self.char_map and random.random() < intensity:\n",
    "                replacements = self.char_map[lower_char]\n",
    "                new_char = random.choice(replacements)\n",
    "                new_text += new_char\n",
    "            else:\n",
    "                new_text += char\n",
    "                \n",
    "        return new_text\n",
    "    \n",
    "    def _synonym_replacement(self, text, intensity):\n",
    "        \"\"\"Replace words with synonyms\"\"\"\n",
    "        words = nltk.word_tokenize(text)\n",
    "        new_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            if len(word) <= 3 or not word.isalpha() or random.random() > intensity:\n",
    "                new_words.append(word)\n",
    "                continue\n",
    "            \n",
    "            # Find synonyms using WordNet\n",
    "            synonyms = []\n",
    "            for syn in wordnet.synsets(word):\n",
    "                for lemma in syn.lemmas():\n",
    "                    if lemma.name() != word.lower():\n",
    "                        synonyms.append(lemma.name())\n",
    "            \n",
    "            if len(synonyms) > 0:\n",
    "                new_word = random.choice(synonyms).replace('_', ' ')\n",
    "                new_words.append(new_word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "                \n",
    "        return ' '.join(new_words)\n",
    "    \n",
    "    def _word_insertion(self, text, intensity):\n",
    "        \"\"\"Insert benign words into text\"\"\"\n",
    "        benign_words = [\"please\", \"kindly\", \"important\", \"notification\", \"information\", \n",
    "                        \"update\", \"confirm\", \"verify\", \"secure\", \"official\"]\n",
    "        \n",
    "        words = text.split()\n",
    "        new_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            new_words.append(word)\n",
    "            if random.random() < intensity:\n",
    "                new_words.append(random.choice(benign_words))\n",
    "                \n",
    "        return ' '.join(new_words)\n",
    "    \n",
    "    def _word_deletion(self, text, intensity):\n",
    "        \"\"\"Delete some words from text\"\"\"\n",
    "        words = text.split()\n",
    "        new_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            if random.random() > intensity:\n",
    "                new_words.append(word)\n",
    "                \n",
    "        return ' '.join(new_words)\n",
    "    \n",
    "    def _change_case(self, text, intensity):\n",
    "        \"\"\"Change case of words or characters\"\"\"\n",
    "        words = text.split()\n",
    "        new_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            if random.random() < intensity:\n",
    "                if random.random() < 0.5:\n",
    "                    # Change to uppercase\n",
    "                    new_word = word.upper()\n",
    "                else:\n",
    "                    # Randomize capitalization\n",
    "                    new_word = ''.join([c.upper() if random.random() < 0.5 else c.lower() for c in word])\n",
    "                new_words.append(new_word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "                \n",
    "        return ' '.join(new_words)\n",
    "    \n",
    "    def _add_html_formatting(self, text, intensity):\n",
    "        \"\"\"Add simple HTML formatting to text\"\"\"\n",
    "        if random.random() < intensity:\n",
    "            # Add basic HTML tags\n",
    "            html_tags = [\n",
    "                (f\"<div style='font-family: Arial;'>{text}</div>\"),\n",
    "                (f\"<p>{text}</p>\"),\n",
    "                (f\"<span style='color: #000000;'>{text}</span>\"),\n",
    "                (f\"<div style='text-align: left;'>{text}</div>\"),\n",
    "                (f\"<font face='Arial'>{text}</font>\")\n",
    "            ]\n",
    "            return random.choice(html_tags)\n",
    "        return text\n",
    "    \n",
    "    def _add_unicode_chars(self, text, intensity):\n",
    "        \"\"\"Add invisible or zero-width unicode characters\"\"\"\n",
    "        # Zero-width characters\n",
    "        zwc = ['\\u200B', '\\u200C', '\\u200D', '\\u200E', '\\u200F', '\\u2060', '\\u2061', '\\u2062', '\\u2063', '\\u2064']\n",
    "        \n",
    "        if random.random() < intensity:\n",
    "            # Insert ZWCs at random positions\n",
    "            chars = list(text)\n",
    "            num_insertions = max(1, int(len(text) * intensity * 0.1))\n",
    "            \n",
    "            for _ in range(num_insertions):\n",
    "                pos = random.randint(0, len(chars))\n",
    "                chars.insert(pos, random.choice(zwc))\n",
    "                \n",
    "            return ''.join(chars)\n",
    "        return text\n",
    "    \n",
    "    def _hide_url_in_text(self, text, intensity):\n",
    "        \"\"\"Replace URLs with text-based hyperlinks\"\"\"\n",
    "        url_pattern = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "        urls = re.findall(url_pattern, text)\n",
    "        \n",
    "        if not urls or random.random() > intensity:\n",
    "            return text\n",
    "        \n",
    "        for url in urls:\n",
    "            mask_texts = [\n",
    "                \"Click here to secure your account\",\n",
    "                \"Verify your information here\",\n",
    "                \"Access your account\",\n",
    "                \"Login now\",\n",
    "                \"Secure login portal\"\n",
    "            ]\n",
    "            mask = random.choice(mask_texts)\n",
    "            text = text.replace(url, mask)\n",
    "            \n",
    "        return text\n",
    "    \n",
    "    def _url_shortener_simulation(self, text, intensity):\n",
    "        \"\"\"Replace URLs with simulated shortened URLs\"\"\"\n",
    "        url_pattern = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "        urls = re.findall(url_pattern, text)\n",
    "        \n",
    "        if not urls or random.random() > intensity:\n",
    "            return text\n",
    "        \n",
    "        for url in urls:\n",
    "            shorteners = [\n",
    "                \"bit.ly/secure-login\",\n",
    "                \"tinyurl.com/account-verify\",\n",
    "                \"goo.gl/auth-portal\",\n",
    "                \"t.co/secure-access\",\n",
    "                \"is.gd/verify-now\"\n",
    "            ]\n",
    "            shortened_url = random.choice(shorteners)\n",
    "            text = text.replace(url, shortened_url)\n",
    "            \n",
    "        return text\n",
    "    \n",
    "    def _html_url_obfuscation(self, text, intensity):\n",
    "        \"\"\"Use HTML to obfuscate URLs\"\"\"\n",
    "        url_pattern = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "        urls = re.findall(url_pattern, text)\n",
    "        \n",
    "        if not urls or random.random() > intensity:\n",
    "            return text\n",
    "        \n",
    "        for url in urls:\n",
    "            obfuscation_methods = [\n",
    "                f\"<a href='{url}'>secure verification link</a>\",\n",
    "                f\"<a href='{url}' style='color:blue; text-decoration:underline;'>click here</a>\",\n",
    "                f\"<a href='{url}'><img src='secure-badge.png' alt='Secure Link'></a>\"\n",
    "            ]\n",
    "            obfuscated_url = random.choice(obfuscation_methods)\n",
    "            text = text.replace(url, obfuscated_url)\n",
    "            \n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },

   "source": [
    "# RL Attacker Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:39:04.927946Z",
     "iopub.status.busy": "2025-05-15T08:39:04.927147Z",
     "iopub.status.idle": "2025-05-15T08:39:04.968404Z",
     "shell.execute_reply": "2025-05-15T08:39:04.967674Z",
     "shell.execute_reply.started": "2025-05-15T08:39:04.927913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RLAttacker:\n",
    "    \"\"\"Reinforcement learning-based attacker that learns to evade the defender\"\"\"\n",
    "    \n",
    "    def __init__(self, perturbation_engine=None):\n",
    "        \"\"\"Initialize RL attacker\n",
    "        \n",
    "        Args:\n",
    "            perturbation_engine: Engine for applying text perturbations\n",
    "        \"\"\"\n",
    "        self.perturbation_engine = perturbation_engine or PerturbationEngine()\n",
    "        \n",
    "        # State representation\n",
    "        self.state_dim = 64  # State embedding dimension\n",
    "        \n",
    "        # Action space setup\n",
    "        self.actions = []\n",
    "        # Character-level actions\n",
    "        self.actions.extend([(func.__name__, func) for func in self.perturbation_engine.char_perturbations])\n",
    "        # Word-level actions\n",
    "        self.actions.extend([(func.__name__, func) for func in self.perturbation_engine.word_perturbations])\n",
    "        # Style actions\n",
    "        self.actions.extend([(func.__name__, func) for func in self.perturbation_engine.style_perturbations])\n",
    "        # URL actions\n",
    "        self.actions.extend([(func.__name__, func) for func in self.perturbation_engine.url_perturbations])\n",
    "        \n",
    "        self.action_dim = len(self.actions)\n",
    "        \n",
    "        # Initialize policy network\n",
    "        self.initialize_model()\n",
    "        \n",
    "        # Experience buffer (for training)\n",
    "        self.buffer = {\n",
    "            'states': [],\n",
    "            'actions': [],\n",
    "            'rewards': [],\n",
    "            'next_states': [],\n",
    "            'dones': []\n",
    "        }\n",
    "        \n",
    "        # Training parameters\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.learning_rate = 5e-4\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        # Tracking metrics\n",
    "        self.successful_attacks = 0\n",
    "        self.total_attacks = 0\n",
    "        \n",
    "    def initialize_model(self):\n",
    "        \"\"\"Initialize policy network model\"\"\"\n",
    "        # Load tokenizer for state representation\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Create model for state embedding\n",
    "        try:\n",
    "            self.encoder = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"distilbert-base-uncased\", num_labels=1)\n",
    "            # Freeze encoder parameters\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading encoder: {e}\")\n",
    "            self.encoder = None\n",
    "        \n",
    "        # Create policy network\n",
    "        class PolicyNetwork(torch.nn.Module):\n",
    "            def __init__(self, state_dim, action_dim):\n",
    "                super(PolicyNetwork, self).__init__()\n",
    "                self.fc1 = torch.nn.Linear(state_dim, 128)\n",
    "                self.fc2 = torch.nn.Linear(128, 64)\n",
    "                self.fc3 = torch.nn.Linear(64, action_dim)\n",
    "                \n",
    "            def forward(self, x):\n",
    "                x = F.relu(self.fc1(x))\n",
    "                x = F.relu(self.fc2(x))\n",
    "                x = self.fc3(x)\n",
    "                return F.softmax(x, dim=1)\n",
    "        \n",
    "        self.policy_net = PolicyNetwork(self.state_dim, self.action_dim)\n",
    "        self.policy_net.to(device)\n",
    "        \n",
    "        print(\"Initialized RL attacker policy network\")\n",
    "    \n",
    "    def get_state_embedding(self, email_text):\n",
    "        \"\"\"Convert email text to state embedding\"\"\"\n",
    "        if self.encoder is None:\n",
    "            # If no encoder, use a simple embedding\n",
    "            # Count features as a basic state representation\n",
    "            state = np.zeros(self.state_dim)\n",
    "            words = email_text.split()\n",
    "            \n",
    "            # Basic text features\n",
    "            state[0] = len(words)  # Word count\n",
    "            state[1] = len(email_text)  # Character count\n",
    "            state[2] = email_text.count('http')  # URL count\n",
    "            state[3] = sum(1 for c in email_text if c.isupper()) / max(1, len(email_text))  # Caps ratio\n",
    "            state[4] = sum(1 for w in words if len(w) > 10) / max(1, len(words))  # Long word ratio\n",
    "            \n",
    "            # Fill remaining dimensions with hash of text\n",
    "            text_hash = int(hashlib.md5(email_text.encode()).hexdigest(), 16)\n",
    "            for i in range(5, self.state_dim):\n",
    "                state[i] = (text_hash % 100) / 100.0\n",
    "                text_hash = text_hash // 10\n",
    "                \n",
    "            return torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Use transformer encoder for state embedding\n",
    "        with torch.no_grad():\n",
    "            inputs = self.tokenizer(\n",
    "                email_text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = self.encoder(**inputs)\n",
    "            # Use the [CLS] token representation\n",
    "            state_embedding = outputs.logits\n",
    "            \n",
    "            # Ensure the embedding is the right dimension\n",
    "            if state_embedding.shape[1] != self.state_dim:\n",
    "                # Add a projection layer if needed\n",
    "                projection = torch.nn.Linear(\n",
    "                    state_embedding.shape[1], self.state_dim, \n",
    "                    device=device\n",
    "                )\n",
    "                state_embedding = projection(state_embedding)\n",
    "                \n",
    "            return state_embedding\n",
    "    \n",
    "    def select_action(self, state, explore=True):\n",
    "        \"\"\"Select action from policy network\"\"\"\n",
    "        with torch.no_grad():\n",
    "            action_probs = self.policy_net(state)\n",
    "            \n",
    "            if explore:\n",
    "                # Exploration: Sample from action distribution\n",
    "                action_idx = torch.multinomial(action_probs, 1).item()\n",
    "            else:\n",
    "                # Exploitation: Take highest probability action\n",
    "                action_idx = torch.argmax(action_probs).item()\n",
    "                \n",
    "            return action_idx\n",
    "    \n",
    "    def apply_action(self, email, action_idx, intensity=0.2):\n",
    "        \"\"\"Apply selected action to the email\"\"\"\n",
    "        # Get the action function\n",
    "        action_name, action_func = self.actions[action_idx]\n",
    "        \n",
    "        # Apply the perturbation with the specified intensity\n",
    "        perturbed_email = email.copy()\n",
    "        perturbed_email['text'] = action_func(email['text'], intensity)\n",
    "        perturbed_email['action_applied'] = action_name\n",
    "        \n",
    "        return perturbed_email\n",
    "    \n",
    "    def calculate_reward(self, email, perturbed_email, detector_output, similarity):\n",
    "        \"\"\"Calculate reward for action\"\"\"\n",
    "        # Extract phishing probability\n",
    "        phishing_prob = detector_output.get('phishing_prob', 0.5)\n",
    "        \n",
    "        # Success reward: High reward if we fool the detector\n",
    "        success_reward = 1.0 if phishing_prob < 0.5 else 0.0\n",
    "        \n",
    "        # Progress reward: Partial reward based on how much we reduced prob\n",
    "        progress_reward = 1.0 - phishing_prob \n",
    "        \n",
    "        # Similarity penalty: Penalize too much change\n",
    "        similarity_penalty = 0.5 * (1.0 - similarity)\n",
    "        \n",
    "        # Combine rewards \n",
    "        reward = 0.7 * success_reward + 0.3 * progress_reward - similarity_penalty\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def calculate_similarity(self, original_text, perturbed_text):\n",
    "        \"\"\"Calculate semantic similarity between texts\"\"\"\n",
    "        # Simple Jaccard similarity as a fallback\n",
    "        original_words = set(original_text.lower().split())\n",
    "        perturbed_words = set(perturbed_text.lower().split())\n",
    "        \n",
    "        # Jaccard similarity\n",
    "        if len(original_words.union(perturbed_words)) > 0:\n",
    "            similarity = len(original_words.intersection(perturbed_words)) / len(original_words.union(perturbed_words))\n",
    "        else:\n",
    "            similarity = 0.0\n",
    "        \n",
    "        # Character-level edit distance also factored in\n",
    "        orig_len = max(1, len(original_text))\n",
    "        pert_len = max(1, len(perturbed_text))\n",
    "        \n",
    "        # Simple normalized edit distance approximation \n",
    "        diff_chars = abs(orig_len - pert_len)\n",
    "        char_similarity = 1.0 - (diff_chars / max(orig_len, pert_len))\n",
    "        \n",
    "        # Combine word and character similarity\n",
    "        combined = 0.7 * similarity + 0.3 * char_similarity\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        self.buffer['states'].append(state)\n",
    "        self.buffer['actions'].append(action)\n",
    "        self.buffer['rewards'].append(reward)\n",
    "        self.buffer['next_states'].append(next_state)\n",
    "        self.buffer['dones'].append(done)\n",
    "        \n",
    "        # Limit buffer size\n",
    "        max_buffer = 1000\n",
    "        if len(self.buffer['states']) > max_buffer:\n",
    "            for key in self.buffer:\n",
    "                self.buffer[key] = self.buffer[key][-max_buffer:]\n",
    "    \n",
    "    def update_policy(self, batch_size=32):\n",
    "        \"\"\"Update policy network from experience buffer\"\"\"\n",
    "        if len(self.buffer['states']) < batch_size:\n",
    "            return 0.0  # Not enough samples for training\n",
    "        \n",
    "        # Sample random batch\n",
    "        indices = np.random.choice(len(self.buffer['states']), batch_size, replace=False)\n",
    "        \n",
    "        states = torch.cat([self.buffer['states'][i] for i in indices], dim=0)\n",
    "        actions = torch.tensor([self.buffer['actions'][i] for i in indices], dtype=torch.long).to(device)\n",
    "        rewards = torch.tensor([self.buffer['rewards'][i] for i in indices], dtype=torch.float32).to(device)\n",
    "        next_states = torch.cat([self.buffer['next_states'][i] for i in indices], dim=0)\n",
    "        dones = torch.tensor([self.buffer['dones'][i] for i in indices], dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Calculate policy loss (REINFORCE with baseline)\n",
    "        action_probs = self.policy_net(states)\n",
    "        selected_probs = action_probs.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Log probabilities and calculate loss\n",
    "        log_probs = torch.log(selected_probs + 1e-10)\n",
    "        loss = -(log_probs * rewards).mean()\n",
    "        \n",
    "        # Update network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def generate_attack(self, emails, defender, max_steps=50):\n",
    "        \"\"\"Generate adversarial emails\"\"\"\n",
    "        adversarial_emails = []\n",
    "        \n",
    "        for email in emails:\n",
    "            # Start with the original email\n",
    "            current_email = email.copy()\n",
    "            current_email['original_text'] = email['text']\n",
    "            current_email['steps'] = 0\n",
    "            \n",
    "            # Initial state\n",
    "            state = self.get_state_embedding(current_email['text'])\n",
    "            \n",
    "            # Attempt to generate an adversarial example\n",
    "            for step in range(max_steps):\n",
    "                # Select and apply action\n",
    "                action_idx = self.select_action(state)\n",
    "                perturbed_email = self.apply_action(current_email, action_idx)\n",
    "                perturbed_email['steps'] = step + 1\n",
    "                \n",
    "                # Get defender's response\n",
    "                detector_result = defender.predict([perturbed_email])[0]\n",
    "                \n",
    "                # Calculate similarity and reward\n",
    "                similarity = self.calculate_similarity(\n",
    "                    email['text'], perturbed_email['text']\n",
    "                )\n",
    "                reward = self.calculate_reward(\n",
    "                    email, perturbed_email, detector_result, similarity\n",
    "                )\n",
    "                \n",
    "                # Get next state\n",
    "                next_state = self.get_state_embedding(perturbed_email['text'])\n",
    "                \n",
    "                # Store experience\n",
    "                done = (detector_result.get('phishing_prob', 1.0) < 0.5) or (step == max_steps - 1)\n",
    "                self.store_experience(state, action_idx, reward, next_state, done)\n",
    "                \n",
    "                # Update for next step\n",
    "                state = next_state\n",
    "                current_email = perturbed_email\n",
    "                \n",
    "                # Break if attack is successful\n",
    "                if detector_result.get('phishing_prob', 1.0) < 0.5:\n",
    "                    self.successful_attacks += 1\n",
    "                    break\n",
    "            \n",
    "            # Track attempts\n",
    "            self.total_attacks += 1\n",
    "            \n",
    "            # Add the final email to the results\n",
    "            current_email['success'] = detector_result.get('phishing_prob', 1.0) < 0.5\n",
    "            current_email['final_prob'] = detector_result.get('phishing_prob', 1.0)\n",
    "            adversarial_emails.append(current_email)\n",
    "            \n",
    "            # Update policy periodically\n",
    "            if len(self.buffer['states']) >= 64:\n",
    "                self.update_policy()\n",
    "        \n",
    "        # Final policy update after all emails\n",
    "        if len(self.buffer['states']) >= 32:\n",
    "            self.update_policy()\n",
    "            \n",
    "        return adversarial_emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaptiveAttacker Implementation (Fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:39:10.559589Z",
     "iopub.status.busy": "2025-05-15T08:39:10.559091Z",
     "iopub.status.idle": "2025-05-15T08:39:10.574424Z",
     "shell.execute_reply": "2025-05-15T08:39:10.573652Z",
     "shell.execute_reply.started": "2025-05-15T08:39:10.559564Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AdaptiveAttacker:\n",
    "    \"\"\"Learning-based attacker that adapts to defender's behavior\"\"\"\n",
    "    \n",
    "    def __init__(self, base_generator=None, base_perturbation=None):\n",
    "        \"\"\"Initialize adaptive attacker\"\"\"\n",
    "        self.template_generator = base_generator or TemplateGenerator()\n",
    "        self.perturbation_engine = base_perturbation or PerturbationEngine()\n",
    "        \n",
    "        # Track successful and failed attack patterns\n",
    "        self.successful_patterns = []  # Patterns that evaded detection\n",
    "        self.failed_patterns = []      # Patterns that were detected\n",
    "        \n",
    "        # Initialize a simple neural language model for adaptive attacks\n",
    "        self.initialize_model()\n",
    "        \n",
    "    def initialize_model(self):\n",
    "        \"\"\"Initialize a simplified language model for adaptive attacks\"\"\"\n",
    "        try:\n",
    "            # Load a small pre-trained model\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"distilbert-base-uncased\", num_labels=2)\n",
    "            self.model.to(device)\n",
    "            \n",
    "            # Initialize with random weights - this is just for demonstration\n",
    "            for param in self.model.parameters():\n",
    "                param.data = torch.randn_like(param.data) * 0.01\n",
    "                \n",
    "            print(\"Initialized adaptive attacker model\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing model: {e}\")\n",
    "            self.tokenizer = None\n",
    "            self.model = None\n",
    "            print(\"Using template-based attacks only\") \n",
    "\n",
    "    def generate_attack(self, n_samples=10, strategy=None):\n",
    "        \"\"\"Generate attack emails\"\"\"\n",
    "        if strategy is None:\n",
    "            # Choose a strategy based on past successes\n",
    "            if len(self.successful_patterns) > 0 and random.random() < 0.7:\n",
    "                # Use successful strategies more often\n",
    "                strategy = random.choice([p['strategy'] for p in self.successful_patterns])\n",
    "            else:\n",
    "                strategy = random.choice(['template', 'perturbation', 'adaptive', 'mixed'])\n",
    "        \n",
    "        if strategy == 'template':\n",
    "            # Simple template-based attack\n",
    "            template_emails = self.template_generator.generate(n_samples)\n",
    "            \n",
    "            # Add strategy tag to each email\n",
    "            for email in template_emails:\n",
    "                email['strategy'] = 'template'\n",
    "                \n",
    "            return template_emails\n",
    "            \n",
    "        elif strategy == 'perturbation':\n",
    "            # Apply perturbations to template emails\n",
    "            base_emails = self.template_generator.generate(n_samples)\n",
    "            perturbed_emails = []\n",
    "            \n",
    "            for email in base_emails:\n",
    "                # Choose a random perturbation technique\n",
    "                perturbed = self.perturbation_engine.perturb(email, intensity=random.uniform(0.1, 0.5))\n",
    "                perturbed['strategy'] = 'perturbation'\n",
    "                perturbed_emails.append(perturbed)\n",
    "                \n",
    "            return perturbed_emails\n",
    "            \n",
    "        elif strategy == 'adaptive':\n",
    "            # Use language model to generate variations\n",
    "            if self.model is None:\n",
    "                # Fall back to perturbation if model isn't available\n",
    "                return self.generate_attack(n_samples, 'perturbation')\n",
    "            \n",
    "            base_emails = self.template_generator.generate(n_samples)\n",
    "            adaptive_emails = []\n",
    "            \n",
    "            for email in base_emails:\n",
    "                # Apply a random sequence of perturbations\n",
    "                perturbed = email.copy()\n",
    "                num_perturbations = random.randint(1, 3)\n",
    "                \n",
    "                for _ in range(num_perturbations):\n",
    "                    # Avoid perturbations that frequently fail\n",
    "                    fail_funcs = [p['perturbation'] for p in self.failed_patterns[-10:]] if self.failed_patterns else []\n",
    "                    \n",
    "                    all_perturbations = (self.perturbation_engine.char_perturbations + \n",
    "                                         self.perturbation_engine.word_perturbations + \n",
    "                                         self.perturbation_engine.style_perturbations + \n",
    "                                         self.perturbation_engine.url_perturbations)\n",
    "                    \n",
    "                    # Filter out frequently failing perturbations if we have enough data\n",
    "                    if len(fail_funcs) >= 5:\n",
    "                        available_perturbations = [f for f in all_perturbations \n",
    "                                               if f.__name__ not in fail_funcs[:5]]\n",
    "                    else:\n",
    "                        available_perturbations = all_perturbations\n",
    "                    \n",
    "                    if not available_perturbations:\n",
    "                        available_perturbations = all_perturbations\n",
    "                        \n",
    "                    technique = random.choice(available_perturbations)\n",
    "                    perturbed = self.perturbation_engine.perturb(\n",
    "                        perturbed, technique=technique, intensity=random.uniform(0.1, 0.5))\n",
    "                \n",
    "                perturbed['strategy'] = 'adaptive'\n",
    "                adaptive_emails.append(perturbed)\n",
    "                \n",
    "            return adaptive_emails\n",
    "            \n",
    "        elif strategy == 'mixed':\n",
    "            # Mix different attack strategies\n",
    "            attacks_per_strategy = n_samples // 3\n",
    "            remainder = n_samples % 3\n",
    "            \n",
    "            template_attacks = self.generate_attack(attacks_per_strategy, 'template')\n",
    "            perturb_attacks = self.generate_attack(attacks_per_strategy, 'perturbation')\n",
    "            adaptive_attacks = self.generate_attack(attacks_per_strategy + remainder, 'adaptive')\n",
    "            \n",
    "            mixed_attacks = template_attacks + perturb_attacks + adaptive_attacks\n",
    "            random.shuffle(mixed_attacks)\n",
    "            \n",
    "            return mixed_attacks\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown attack strategy: {strategy}\")\n",
    "                 \n",
    "    def update_model(self, attack_results):\n",
    "        \"\"\"Update model based on attack results\"\"\"\n",
    "        # Add attack patterns to our history\n",
    "        for result in attack_results:\n",
    "            email = result.get('email', result)\n",
    "            was_detected = result.get('detected', False)\n",
    "            \n",
    "            pattern = {\n",
    "                'text': email['text'],\n",
    "                'strategy': email.get('strategy', 'unknown'),\n",
    "                'perturbation': email.get('perturbation', None)\n",
    "            }\n",
    "            \n",
    "            if was_detected:\n",
    "                self.failed_patterns.append(pattern)\n",
    "            else:\n",
    "                self.successful_patterns.append(pattern)\n",
    "        \n",
    "        print(f\"Updated attacker model with {len(attack_results)} results\")\n",
    "        print(f\"Successful evasions: {sum(1 for r in attack_results if not r['detected'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game-Theoretic Planner Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:39:13.103369Z",
     "iopub.status.busy": "2025-05-15T08:39:13.102769Z",
     "iopub.status.idle": "2025-05-15T08:39:13.114684Z",
     "shell.execute_reply": "2025-05-15T08:39:13.114115Z",
     "shell.execute_reply.started": "2025-05-15T08:39:13.103347Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GameTheoreticPlanner:\n",
    "    \"\"\"Plan attacks using game theory\"\"\"\n",
    "    \n",
    "    def __init__(self, attack_strategies=None):\n",
    "        \"\"\"Initialize game-theoretic attack planner\"\"\"\n",
    "        self.attack_strategies = attack_strategies or [\n",
    "            'template', 'perturbation', 'adaptive', 'mixed'\n",
    "        ]\n",
    "        \n",
    "        # Payoff matrix: strategy x strategy -> utility\n",
    "        self.payoff_matrix = {\n",
    "            strategy: {defense: 0.5 for defense in self.attack_strategies}\n",
    "            for strategy in self.attack_strategies\n",
    "        }\n",
    "        \n",
    "        # Initialize strategy distribution (uniform)\n",
    "        self.strategy_distribution = {\n",
    "            strategy: 1.0 / len(self.attack_strategies)\n",
    "            for strategy in self.attack_strategies\n",
    "        }\n",
    "    \n",
    "    def update_payoffs(self, attack_results):\n",
    "        \"\"\"Update payoff matrix based on attack results\"\"\"\n",
    "        # Group results by strategy\n",
    "        strategy_results = {}\n",
    "    \n",
    "        for result in attack_results:\n",
    "            try:\n",
    "                email = result.get('email', result)\n",
    "                strategy = email.get('strategy', 'unknown')\n",
    "                \n",
    "                if strategy not in strategy_results:\n",
    "                    strategy_results[strategy] = []\n",
    "                \n",
    "                detected = result.get('detected', False)\n",
    "                strategy_results[strategy].append(detected)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing result in update_payoffs: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Update payoff matrix\n",
    "        for strategy, results in strategy_results.items():\n",
    "            if not results:\n",
    "                continue\n",
    "            \n",
    "            # Check if strategy exists in payoff matrix, add it if not\n",
    "            if strategy not in self.payoff_matrix:\n",
    "                print(f\"Adding new strategy to payoff matrix: {strategy}\")\n",
    "                self.payoff_matrix[strategy] = {defense: 0.5 for defense in self.attack_strategies}\n",
    "                # Also add this strategy as a defense option for all existing strategies\n",
    "                for existing_strategy in self.attack_strategies:\n",
    "                    self.payoff_matrix[existing_strategy][strategy] = 0.5\n",
    "                # Add to list of strategies\n",
    "                self.attack_strategies.append(strategy)\n",
    "                # Add to strategy distribution with small initial probability\n",
    "                total_prob = sum(self.strategy_distribution.values())\n",
    "                # Give it a small initial probability by reducing others slightly\n",
    "                if total_prob > 0:\n",
    "                    reduction_factor = 0.95  # Reduce existing probabilities by 5%\n",
    "                    for s in self.strategy_distribution:\n",
    "                        self.strategy_distribution[s] *= reduction_factor\n",
    "                    # Set new strategy to have 5% probability\n",
    "                    self.strategy_distribution[strategy] = 0.05\n",
    "                else:\n",
    "                    # If total probability is 0 (shouldn't happen normally), give equal probability\n",
    "                    n_strategies = len(self.attack_strategies)\n",
    "                    for s in self.attack_strategies:\n",
    "                        self.strategy_distribution[s] = 1.0 / n_strategies\n",
    "                \n",
    "            # Payoff is the evasion rate (1 - detection rate)\n",
    "            detection_rate = sum(results) / len(results)\n",
    "            evasion_rate = 1 - detection_rate\n",
    "            \n",
    "            # Update payoff for all defense strategies (simplified)\n",
    "            for defense in self.attack_strategies:\n",
    "                # Discount old payoff and add new observation\n",
    "                alpha = 0.3  # Learning rate\n",
    "                old_payoff = self.payoff_matrix[strategy][defense]\n",
    "                new_payoff = (1 - alpha) * old_payoff + alpha * evasion_rate\n",
    "                self.payoff_matrix[strategy][defense] = new_payoff\n",
    "\n",
    "    def solve_mixed_strategy(self):\n",
    "        \"\"\"Solve for Nash equilibrium mixed strategy\"\"\"\n",
    "        # Calculate expected payoffs for each strategy\n",
    "        expected_payoffs = {}\n",
    "        \n",
    "        for attack_strategy in self.attack_strategies:\n",
    "            # Assume defender uses same distribution (zero-sum game)\n",
    "            payoff = sum(\n",
    "                self.payoff_matrix[attack_strategy][defense] * self.strategy_distribution[defense]\n",
    "                for defense in self.attack_strategies\n",
    "            )\n",
    "            expected_payoffs[attack_strategy] = payoff\n",
    "        \n",
    "        # Find best-response strategy\n",
    "        best_payoff = max(expected_payoffs.values())\n",
    "        best_strategies = [\n",
    "            strategy for strategy, payoff in expected_payoffs.items()\n",
    "            if abs(payoff - best_payoff) < 1e-6\n",
    "        ]\n",
    "        \n",
    "        # Update strategy distribution (softmax)\n",
    "        temperature = 0.1  # Exploration parameter\n",
    "        exps = {\n",
    "            strategy: np.exp(expected_payoffs[strategy] / temperature)\n",
    "            for strategy in self.attack_strategies\n",
    "        }\n",
    "        total_exp = sum(exps.values())\n",
    "        \n",
    "        self.strategy_distribution = {\n",
    "            strategy: exp / total_exp\n",
    "            for strategy, exp in exps.items()\n",
    "        }\n",
    "        \n",
    "        return self.strategy_distribution\n",
    "    \n",
    "    def choose_strategy(self):\n",
    "        \"\"\"Choose a strategy based on current distribution\"\"\"\n",
    "        strategies = list(self.strategy_distribution.keys())\n",
    "        probabilities = list(self.strategy_distribution.values())\n",
    "        \n",
    "        return np.random.choice(strategies, p=probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Defender Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:39:15.847744Z",
     "iopub.status.busy": "2025-05-15T08:39:15.847431Z",
     "iopub.status.idle": "2025-05-15T08:39:15.874288Z",
     "shell.execute_reply": "2025-05-15T08:39:15.873644Z",
     "shell.execute_reply.started": "2025-05-15T08:39:15.847721Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmailDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for emails\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        \"\"\"Initialize dataset\"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class TransformerDefender:\n",
    "    \"\"\"Transformer-based phishing email detector\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='distilbert-base-uncased'):\n",
    "        \"\"\"Initialize transformer defender\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=2)\n",
    "        self.model.to(device)\n",
    "        \n",
    "        # Default training parameters\n",
    "        self.batch_size = 8\n",
    "        self.max_length = 512\n",
    "        self.learning_rate = 2e-5\n",
    "        self.weight_decay = 0.01\n",
    "        \n",
    "        # For calibration\n",
    "        self.calibration_temp = 1.0\n",
    "    \n",
    "    def train(self, train_df, val_df, epochs=3):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        # Create datasets\n",
    "        train_dataset = EmailDataset(\n",
    "            train_df['text'].tolist(),\n",
    "            train_df['label'].tolist(),\n",
    "            self.tokenizer,\n",
    "            self.max_length\n",
    "        )\n",
    "        \n",
    "        val_dataset = EmailDataset(\n",
    "            val_df['text'].tolist(),\n",
    "            val_df['label'].tolist(),\n",
    "            self.tokenizer,\n",
    "            self.max_length\n",
    "        )\n",
    "        \n",
    "        # Training arguments\n",
    "        try:\n",
    "            # Try with minimal parameters first\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=f'./models/{self.model_name}-detector',\n",
    "                num_train_epochs=epochs,\n",
    "                per_device_train_batch_size=self.batch_size,\n",
    "                per_device_eval_batch_size=self.batch_size,\n",
    "                warmup_steps=500,\n",
    "                weight_decay=self.weight_decay,\n",
    "                logging_dir='./logs',\n",
    "                logging_steps=10,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Simplified training arguments due to: {e}\")\n",
    "            # Fallback to even more minimal parameters\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=f'./models/{self.model_name}-detector',\n",
    "                num_train_epochs=epochs,\n",
    "            )\n",
    "        \n",
    "        # Metrics function\n",
    "        def compute_metrics(eval_pred):\n",
    "            logits, labels = eval_pred\n",
    "            predictions = np.argmax(logits, axis=-1)\n",
    "            \n",
    "            return {\n",
    "                'accuracy': accuracy_score(labels, predictions),\n",
    "                'precision': precision_score(labels, predictions),\n",
    "                'recall': recall_score(labels, predictions),\n",
    "                'f1': f1_score(labels, predictions)\n",
    "            }\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        train_output = trainer.train()\n",
    "        \n",
    "        # Evaluate\n",
    "        try:\n",
    "            eval_output = trainer.evaluate()\n",
    "            \n",
    "            return {\n",
    "                'train_loss': train_output.metrics.get('train_loss', 0.0),\n",
    "                'eval_loss': eval_output.get('eval_loss', 0.0),\n",
    "                'eval_accuracy': eval_output.get('eval_accuracy', 0.0),\n",
    "                'eval_precision': eval_output.get('eval_precision', 0.0),\n",
    "                'eval_recall': eval_output.get('eval_recall', 0.0),\n",
    "                'eval_f1': eval_output.get('eval_f1', 0.0)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Evaluation error - {e}\")\n",
    "            return {\n",
    "                'train_loss': train_output.metrics.get('train_loss', 0.0),\n",
    "                'eval_loss': 0.0,\n",
    "                'eval_accuracy': 0.0,\n",
    "                'eval_precision': 0.0,\n",
    "                'eval_recall': 0.0,\n",
    "                'eval_f1': 0.0\n",
    "            }\n",
    "    \n",
    "    def predict(self, emails, threshold=0.5, calibrate=False):\n",
    "        \"\"\"Predict phishing probability for emails with temperature scaling\"\"\"\n",
    "        # Extract text from email dictionaries if needed\n",
    "        if isinstance(emails[0], dict):\n",
    "            texts = [email['text'] for email in emails]\n",
    "        else:\n",
    "            texts = emails\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = EmailDataset(\n",
    "            texts,\n",
    "            [0] * len(texts),  # Dummy labels\n",
    "            self.tokenizer,\n",
    "            self.max_length\n",
    "        )\n",
    "        \n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size)\n",
    "        \n",
    "        # Make predictions\n",
    "        self.model.eval()\n",
    "        all_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Move tensors to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Convert to probabilities\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        # Concatenate results\n",
    "        all_probs = np.vstack(all_probs)\n",
    "        phishing_probs = all_probs[:, 1]  # Probability of class 1 (phishing)\n",
    "        \n",
    "        # Apply temperature scaling calibration if requested\n",
    "        if calibrate and hasattr(self, 'calibration_temp'):\n",
    "            # Convert probs to logits\n",
    "            epsilon = 1e-10\n",
    "            phishing_probs_clipped = np.clip(phishing_probs, epsilon, 1 - epsilon)\n",
    "            logits = np.log(phishing_probs_clipped / (1 - phishing_probs_clipped))\n",
    "            \n",
    "            # Apply temperature\n",
    "            scaled_logits = logits / self.calibration_temp\n",
    "            \n",
    "            # Convert back to probabilities\n",
    "            phishing_probs = 1 / (1 + np.exp(-scaled_logits))\n",
    "        \n",
    "        # Create prediction results\n",
    "        results = []\n",
    "        for i, prob in enumerate(phishing_probs):\n",
    "            is_phishing = prob >= threshold\n",
    "            \n",
    "            # Create result dictionary\n",
    "            if isinstance(emails[0], dict):\n",
    "                result = emails[i].copy()\n",
    "            else:\n",
    "                result = {'text': texts[i]}\n",
    "                \n",
    "            result['phishing_prob'] = float(prob)\n",
    "            result['is_phishing'] = bool(is_phishing)\n",
    "            result['detected'] = bool(is_phishing)  # For compatibility with attack results\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _evaluate_on_emails(self, emails):\n",
    "        \"\"\"Evaluate model on a set of emails\"\"\"\n",
    "        # Make predictions\n",
    "        predictions = self.predict(emails, calibrate=True)\n",
    "        \n",
    "        # Extract results\n",
    "        total = len(predictions)\n",
    "        detected = sum(1 for p in predictions if p['detected'])\n",
    "        \n",
    "        # Calculate metrics\n",
    "        detection_rate = detected / total if total > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'total': total,\n",
    "            'detected': detected,\n",
    "            'detection_rate': detection_rate\n",
    "        }\n",
    "    \n",
    "    def calibrate_with_temperature(self, val_df):\n",
    "        \"\"\"Calibrate model probabilities using temperature scaling\"\"\"\n",
    "        # Get raw predictions on validation set\n",
    "        val_preds = self.predict(val_df['text'].tolist(), threshold=0.5, calibrate=False)\n",
    "        val_probs = np.array([pred['phishing_prob'] for pred in val_preds])\n",
    "        val_labels = val_df['label'].values\n",
    "        \n",
    "        # Calculate original ECE\n",
    "        def compute_ece(probs, labels, n_bins=10):\n",
    "            \"\"\"Calculate Expected Calibration Error\"\"\"\n",
    "            bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "            bin_lowers = bin_boundaries[:-1]\n",
    "            bin_uppers = bin_boundaries[1:]\n",
    "            \n",
    "            confidences = probs\n",
    "            predictions = (probs >= 0.5).astype(np.int32)\n",
    "            accuracies = (predictions == labels).astype(np.float32)\n",
    "            \n",
    "            ece = 0.0\n",
    "            for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "                # Find samples in this bin\n",
    "                in_bin = np.logical_and(confidences > bin_lower, confidences <= bin_upper)\n",
    "                prop_in_bin = np.mean(in_bin)\n",
    "                \n",
    "                if prop_in_bin > 0:\n",
    "                    accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "                    avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "                    ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "                    \n",
    "            return ece\n",
    "        \n",
    "        original_ece = compute_ece(val_probs, val_labels)\n",
    "        \n",
    "        # Find optimal temperature using grid search\n",
    "        def temperature_scale(logits, temperature):\n",
    "            \"\"\"Apply temperature scaling to logits\"\"\"\n",
    "            # Convert probabilities back to logits\n",
    "            epsilon = 1e-10\n",
    "            probs = np.clip(logits, epsilon, 1 - epsilon)\n",
    "            logits = np.log(probs / (1 - probs))\n",
    "            \n",
    "            # Apply temperature\n",
    "            scaled_logits = logits / temperature\n",
    "            \n",
    "            # Convert back to probabilities\n",
    "            scaled_probs = 1 / (1 + np.exp(-scaled_logits))\n",
    "            return scaled_probs\n",
    "        \n",
    "        # Grid search for optimal temperature\n",
    "        temperatures = np.linspace(0.5, 3.0, 26)  # 0.5 to 3.0 in steps of 0.1\n",
    "        best_ece = float('inf')\n",
    "        best_temperature = 1.0\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            scaled_probs = temperature_scale(val_probs, temp)\n",
    "            ece = compute_ece(scaled_probs, val_labels)\n",
    "            \n",
    "            if ece < best_ece:\n",
    "                best_ece = ece\n",
    "                best_temperature = temp\n",
    "        \n",
    "        print(f\"Original ECE: {original_ece:.4f}\")\n",
    "        print(f\"Best temperature: {best_temperature:.4f}\")\n",
    "        print(f\"Calibrated ECE: {best_ece:.4f}\")\n",
    "        \n",
    "        # Store the optimal temperature\n",
    "        self.calibration_temp = best_temperature\n",
    "        \n",
    "        # Return calibration metrics\n",
    "        return {\n",
    "            'original_ece': original_ece,\n",
    "            'calibrated_ece': best_ece,\n",
    "            'temperature': best_temperature,\n",
    "            'improvement_percent': (original_ece - best_ece) / original_ece * 100\n",
    "        }\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save model and tokenizer\"\"\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        self.model.save_pretrained(path)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        \n",
    "        # Save calibrator if available\n",
    "        if hasattr(self, 'calibration_temp'):\n",
    "            import pickle\n",
    "            with open(os.path.join(path, 'calibrator.pkl'), 'wb') as f:\n",
    "                pickle.dump(self.calibration_temp, f)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load model and tokenizer\"\"\"\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        self.model.to(device)\n",
    "        \n",
    "        # Load calibrator if available\n",
    "        calibrator_path = os.path.join(path, 'calibrator.pkl')\n",
    "        if os.path.exists(calibrator_path):\n",
    "            import pickle\n",
    "            with open(calibrator_path, 'rb') as f:\n",
    "                self.calibration_temp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Defender, Adversarial Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:39:28.272311Z",
     "iopub.status.busy": "2025-05-15T08:39:28.271690Z",
     "iopub.status.idle": "2025-05-15T08:39:28.298070Z",
     "shell.execute_reply": "2025-05-15T08:39:28.297465Z",
     "shell.execute_reply.started": "2025-05-15T08:39:28.272288Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AdaptiveDefender:\n",
    "    \"\"\"Defender that adapts to attacks over time\"\"\"\n",
    "    \n",
    "    def __init__(self, base_defender=None):\n",
    "        \"\"\"Initialize adaptive defender\"\"\"\n",
    "        # Initialize with a transformer defender if none provided\n",
    "        self.defender = base_defender or TransformerDefender()\n",
    "        \n",
    "        # Track attack history\n",
    "        self.attack_history = []\n",
    "        \n",
    "        # Initialize memory buffer for online learning\n",
    "        self.memory_buffer = {\n",
    "            'texts': [],\n",
    "            'labels': []\n",
    "        }\n",
    "        self.buffer_size = 1000  # Maximum examples to keep in memory\n",
    "    \n",
    "    def predict(self, emails, threshold=0.5):\n",
    "        \"\"\"Predict phishing probability\"\"\"\n",
    "        # Forward to the current defender\n",
    "        return self.defender.predict(emails, threshold=threshold, calibrate=True)\n",
    "    \n",
    "    def update(self, attack_results, retrain=True):\n",
    "        \"\"\"Update defender based on attack results\"\"\"\n",
    "        # Add results to attack history\n",
    "        self.attack_history.extend(attack_results)\n",
    "        \n",
    "        # Add to memory buffer\n",
    "        for result in attack_results:\n",
    "            email = result['text'] if isinstance(result, dict) and 'text' in result else result['email']['text']\n",
    "            # Phishing emails should have label 1\n",
    "            label = 1 if 'label' in result and result['label'] == 1 else 1\n",
    "            \n",
    "            self.memory_buffer['texts'].append(email)\n",
    "            self.memory_buffer['labels'].append(label)\n",
    "        \n",
    "        # Trim buffer if it gets too large (keep most recent examples)\n",
    "        if len(self.memory_buffer['texts']) > self.buffer_size:\n",
    "            self.memory_buffer['texts'] = self.memory_buffer['texts'][-self.buffer_size:]\n",
    "            self.memory_buffer['labels'] = self.memory_buffer['labels'][-self.buffer_size:]\n",
    "        \n",
    "        # Retrain if requested\n",
    "        if retrain and len(self.memory_buffer['texts']) >= 50:\n",
    "            # Create a small dataset for fine-tuning\n",
    "            df = pd.DataFrame({\n",
    "                'text': self.memory_buffer['texts'],\n",
    "                'label': self.memory_buffer['labels']\n",
    "            })\n",
    "            \n",
    "            # Split into train/val\n",
    "            train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "            \n",
    "            # Fine-tune the model\n",
    "            metrics = self.defender.train(train_df, val_df, epochs=1)\n",
    "            \n",
    "            # Re-calibrate\n",
    "            self.defender.calibrate_with_temperature(val_df)\n",
    "            \n",
    "            return {\n",
    "                'update_size': len(attack_results),\n",
    "                'memory_buffer_size': len(self.memory_buffer['texts']),\n",
    "                'retrain_metrics': metrics\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'update_size': len(attack_results),\n",
    "            'memory_buffer_size': len(self.memory_buffer['texts']),\n",
    "            'retrain': False\n",
    "        }\n",
    "\n",
    "class AdversarialController:\n",
    "    \"\"\"Controller for the adversarial loop with proper convergence checking\"\"\"\n",
    "    \n",
    "    def __init__(self, attacker=None, defender=None, planner=None):\n",
    "        \"\"\"Initialize controller\"\"\"\n",
    "        self.attacker = attacker\n",
    "        self.defender = defender \n",
    "        self.planner = planner \n",
    "        \n",
    "        self.round_history = []\n",
    "        self.current_round = 0\n",
    "        \n",
    "        # For convergence checking\n",
    "        self.previous_robust_acc = 0.0\n",
    "        self.convergence_threshold = 0.02  # Stop if improvement < 2%\n",
    "        self.convergence_patience = 2      # Number of rounds with small improvement\n",
    "        self.patience_counter = 0\n",
    "        \n",
    "        # Create required directories\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "    \n",
    "    def run_round(self, n_samples=10, strategy=None):\n",
    "        \"\"\"Run a single round in the adversarial loop\"\"\"\n",
    "        self.current_round += 1\n",
    "        print(f\"\\n===== Starting Round {self.current_round} =====\")\n",
    "        \n",
    "        # Choose attack strategy if not specified\n",
    "        if strategy is None and self.planner:\n",
    "            strategy = self.planner.choose_strategy()\n",
    "            print(f\"Planner chose strategy: {strategy}\")\n",
    "        \n",
    "        # Generate attack emails\n",
    "        attack_emails = self.attacker.generate_attack(n_samples, strategy)\n",
    "        print(f\"Generated {len(attack_emails)} attack emails\")\n",
    "        \n",
    "        # Defender labels the emails\n",
    "        attack_results = self.defender.predict(attack_emails)\n",
    "        print(f\"Defender processed the attacks\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = self._calculate_metrics(attack_results)\n",
    "        print(f\"Attack metrics - Evasion rate: {metrics['evasion_rate']:.2f}, \"\n",
    "              f\"Detection rate: {metrics['detection_rate']:.2f}\")\n",
    "        \n",
    "        # Check for convergence\n",
    "        current_robust_acc = metrics['detection_rate']\n",
    "        improvement = current_robust_acc - self.previous_robust_acc\n",
    "        \n",
    "        print(f\"Improvement in robust accuracy: {improvement:.4f}\")\n",
    "        \n",
    "        converged = False\n",
    "        if self.current_round > 1:\n",
    "            if abs(improvement) < self.convergence_threshold:\n",
    "                self.patience_counter += 1\n",
    "                if self.patience_counter >= self.convergence_patience:\n",
    "                    print(f\"Converged after {self.current_round} rounds - robust accuracy stabilized\")\n",
    "                    converged = True\n",
    "            else:\n",
    "                self.patience_counter = 0\n",
    "        \n",
    "        self.previous_robust_acc = current_robust_acc\n",
    "        \n",
    "        # Update the game-theoretic planner\n",
    "        if self.planner:\n",
    "            self.planner.update_payoffs(attack_results)\n",
    "            new_distribution = self.planner.solve_mixed_strategy()\n",
    "            print(f\"Updated strategy distribution: {new_distribution}\")\n",
    "        \n",
    "        # Save attack results for future analysis\n",
    "        results_with_attacks = {\n",
    "            'round': self.current_round,\n",
    "            'strategy': strategy,\n",
    "            'attack_size': len(attack_emails),\n",
    "            'metrics': metrics,\n",
    "            'timestamp': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'attack_results': attack_results,  # Store full results\n",
    "            'converged': converged\n",
    "        }\n",
    "        \n",
    "        # Log round results (without full attack data to save space)\n",
    "        round_results = {\n",
    "            'round': self.current_round,\n",
    "            'strategy': strategy,\n",
    "            'attack_size': len(attack_emails),\n",
    "            'metrics': metrics,\n",
    "            'timestamp': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'converged': converged\n",
    "        }\n",
    "        self.round_history.append(round_results)\n",
    "        \n",
    "        # Save round results\n",
    "        self._save_round_results(round_results)\n",
    "        \n",
    "        return results_with_attacks, converged\n",
    "    \n",
    "    def adapt_and_update(self, round_results, adapt_attacker=True, adapt_defender=True):\n",
    "        \"\"\"Update attacker and defender based on round results\"\"\"\n",
    "        print(f\"\\n===== Adapting after Round {self.current_round} =====\")\n",
    "        \n",
    "        update_metrics = {}\n",
    "        \n",
    "        # Update attacker if required\n",
    "        if adapt_attacker and hasattr(self.attacker, 'update_model'):\n",
    "            attack_results = round_results.get('attack_results', [])\n",
    "            if not attack_results:\n",
    "                # Reconstruct from metrics\n",
    "                evasion_count = int(round_results['metrics']['evasion_rate'] * \n",
    "                                   round_results['attack_size'])\n",
    "                \n",
    "                # Simulate attack results\n",
    "                attack_results = [\n",
    "                    {'detected': i >= evasion_count, \n",
    "                     'email': {'strategy': round_results['strategy']}}\n",
    "                    for i in range(round_results['attack_size'])\n",
    "                ]\n",
    "            \n",
    "            self.attacker.update_model(attack_results)\n",
    "            print(\"Attacker model updated\")\n",
    "            update_metrics['attacker_updated'] = True\n",
    "        \n",
    "        # Update defender if required\n",
    "        if adapt_defender and hasattr(self.defender, 'update'):\n",
    "            attack_results = round_results.get('attack_results', [])\n",
    "            defender_metrics = self.defender.update(attack_results, retrain=True)\n",
    "            print(\"Defender updated\")\n",
    "            update_metrics['defender_updated'] = True\n",
    "            update_metrics['defender_metrics'] = defender_metrics\n",
    "        \n",
    "        return update_metrics\n",
    "    \n",
    "    def _calculate_metrics(self, attack_results):\n",
    "        \"\"\"Calculate metrics for attack results\"\"\"\n",
    "        # Calculate basic metrics\n",
    "        total = len(attack_results)\n",
    "        detected = sum(1 for result in attack_results if result.get('detected', False))\n",
    "        evaded = total - detected\n",
    "        \n",
    "        detection_rate = detected / total if total > 0 else 0\n",
    "        evasion_rate = evaded / total if total > 0 else 0\n",
    "        \n",
    "        # Group by strategy if available\n",
    "        strategy_metrics = {}\n",
    "        for result in attack_results:\n",
    "            # Extract strategy from email or result\n",
    "            strategy = None\n",
    "            if 'strategy' in result:\n",
    "                strategy = result['strategy']\n",
    "            elif 'email' in result and isinstance(result['email'], dict) and 'strategy' in result['email']:\n",
    "                strategy = result['email']['strategy']\n",
    "                \n",
    "            if strategy:\n",
    "                if strategy not in strategy_metrics:\n",
    "                    strategy_metrics[strategy] = {'total': 0, 'detected': 0}\n",
    "                \n",
    "                strategy_metrics[strategy]['total'] += 1\n",
    "                if result.get('detected', False):\n",
    "                    strategy_metrics[strategy]['detected'] += 1\n",
    "        \n",
    "        # Calculate per-strategy metrics\n",
    "        for strategy, counts in strategy_metrics.items():\n",
    "            if counts['total'] > 0:\n",
    "                counts['detection_rate'] = counts['detected'] / counts['total']\n",
    "                counts['evasion_rate'] = 1 - counts['detection_rate']\n",
    "        \n",
    "        return {\n",
    "            'total': total,\n",
    "            'detected': detected,\n",
    "            'evaded': evaded,\n",
    "            'detection_rate': detection_rate,\n",
    "            'evasion_rate': evasion_rate,\n",
    "            'strategy_metrics': strategy_metrics\n",
    "        }\n",
    "    \n",
    "    def _save_round_results(self, round_results):\n",
    "        \"\"\"Save round results to disk\"\"\"\n",
    "        import json\n",
    "        \n",
    "        # Save to JSON file\n",
    "        filename = f\"results/round_{round_results['round']}.json\"\n",
    "        with open(filename, 'w') as f:\n",
    "            # Clone dictionary to avoid modifying the original\n",
    "            results_copy = round_results.copy()\n",
    "            \n",
    "            # Remove attack_results as they can be large and contain objects\n",
    "            if 'attack_results' in results_copy:\n",
    "                del results_copy['attack_results']\n",
    "            \n",
    "            json.dump(results_copy, f, indent=2, default=str)\n",
    "    \n",
    "    def run_adversarial_loop(self, n_rounds=5, samples_per_round=10, \n",
    "                           adapt_attacker=True, adapt_defender=True):\n",
    "        \"\"\"Run the complete adversarial loop\"\"\"\n",
    "        for round_idx in range(n_rounds):\n",
    "            try:\n",
    "                # Run a round\n",
    "                round_results, converged = self.run_round(samples_per_round)\n",
    "                \n",
    "                # Update attacker and defender\n",
    "                if round_idx < n_rounds - 1 and not converged:  # Don't update after the last round or if converged\n",
    "                    self.adapt_and_update(round_results, adapt_attacker, adapt_defender)\n",
    "                \n",
    "                # Break if converged\n",
    "                if converged:\n",
    "                    print(f\"Converged after {round_idx+1} rounds. Stopping early.\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in round {round_idx+1}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()  # This will print the full stack trace\n",
    "                print(\"Continuing to next round...\")\n",
    "        \n",
    "        return self.round_history\n",
    "    \n",
    "    def evaluate(self, test_df=None):\n",
    "        \"\"\"Evaluate current defender on test data\"\"\"\n",
    "        if test_df is None or len(test_df) == 0:\n",
    "            print(\"No test data provided for evaluation\")\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            # Make predictions\n",
    "            predictions = self.defender.predict(test_df['text'].tolist())\n",
    "            pred_labels = [1 if pred.get('is_phishing', False) else 0 for pred in predictions]\n",
    "            true_labels = test_df['label'].tolist()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = {\n",
    "                'accuracy': accuracy_score(true_labels, pred_labels),\n",
    "                'precision': precision_score(true_labels, pred_labels),\n",
    "                'recall': recall_score(true_labels, pred_labels),\n",
    "                'f1': f1_score(true_labels, pred_labels),\n",
    "                'roc_auc': roc_auc_score(true_labels, [pred.get('phishing_prob', 0.5) for pred in predictions])\n",
    "            }\n",
    "            \n",
    "            # Confusion matrix\n",
    "            cm = confusion_matrix(true_labels, pred_labels)\n",
    "            metrics['confusion_matrix'] = cm.tolist()\n",
    "            \n",
    "            return metrics\n",
    "        except Exception as e:\n",
    "            print(f\"Error in evaluation: {str(e)}\")\n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'accuracy': 0.0,\n",
    "                'precision': 0.0,\n",
    "                'recall': 0.0,\n",
    "                'f1': 0.0,\n",
    "                'roc_auc': 0.5\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:39:34.249484Z",
     "iopub.status.busy": "2025-05-15T08:39:34.248725Z",
     "iopub.status.idle": "2025-05-15T08:39:34.276738Z",
     "shell.execute_reply": "2025-05-15T08:39:34.275972Z",
     "shell.execute_reply.started": "2025-05-15T08:39:34.249454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics_over_rounds(controller):\n",
    "    \"\"\"Plot metrics evolution over rounds\"\"\"\n",
    "    if not controller.round_history:\n",
    "        print(\"No rounds to plot\")\n",
    "        return\n",
    "    \n",
    "    # Extract data\n",
    "    rounds = [r['round'] for r in controller.round_history]\n",
    "    detection_rates = [r['metrics']['detection_rate'] for r in controller.round_history]\n",
    "    evasion_rates = [r['metrics']['evasion_rate'] for r in controller.round_history]\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(rounds, detection_rates, 'b-o', label='Detection Rate')\n",
    "    plt.plot(rounds, evasion_rates, 'r-o', label='Evasion Rate')\n",
    "    plt.xlabel('Round')\n",
    "    plt.ylabel('Rate')\n",
    "    plt.title('Detection and Evasion Rates over Rounds')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rounds)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add confidence intervals (simplified)\n",
    "    for i, r in enumerate(rounds):\n",
    "        # Assuming a binomial distribution for success/failure\n",
    "        n = controller.round_history[i]['attack_size']\n",
    "        p_detect = detection_rates[i]\n",
    "        std_dev = np.sqrt((p_detect * (1 - p_detect)) / n)\n",
    "        ci = 1.96 * std_dev  # 95% confidence interval\n",
    "        \n",
    "        plt.errorbar(r, p_detect, yerr=ci, fmt='none', ecolor='blue', capsize=5, alpha=0.5)\n",
    "    \n",
    "    plt.savefig('results/metrics_evolution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot strategy-specific metrics if available\n",
    "    strategies = set()\n",
    "    for round_result in controller.round_history:\n",
    "        if 'strategy_metrics' in round_result['metrics']:\n",
    "            strategies.update(round_result['metrics']['strategy_metrics'].keys())\n",
    "    \n",
    "    if strategies:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        for strategy in sorted(strategies):\n",
    "            strategy_evasion = []\n",
    "            \n",
    "            for r in controller.round_history:\n",
    "                metrics = r['metrics']\n",
    "                if ('strategy_metrics' in metrics and \n",
    "                    strategy in metrics['strategy_metrics'] and\n",
    "                    'evasion_rate' in metrics['strategy_metrics'][strategy]):\n",
    "                    strategy_evasion.append(metrics['strategy_metrics'][strategy]['evasion_rate'])\n",
    "                else:\n",
    "                    strategy_evasion.append(None)  # Missing data point\n",
    "            \n",
    "            # Plot only if we have data\n",
    "            valid_points = [(r, e) for r, e in zip(rounds, strategy_evasion) if e is not None]\n",
    "            if valid_points:\n",
    "                x, y = zip(*valid_points)\n",
    "                plt.plot(x, y, 'o-', label=f'Strategy: {strategy}')\n",
    "        \n",
    "        plt.xlabel('Round')\n",
    "        plt.ylabel('Evasion Rate')\n",
    "        plt.title('Evasion Rate by Strategy')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.savefig('results/strategy_performance.png')\n",
    "        plt.close()\n",
    "\n",
    "def plot_reliability_diagram(predictions, labels, title=\"Reliability Diagram\", calibrated=False):\n",
    "    \"\"\"Plot a reliability diagram for model calibration\"\"\"\n",
    "    # Configure plot\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # Calculate reliability\n",
    "    n_bins = 10\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    \n",
    "    # Compute ECE\n",
    "    ece = 0.0\n",
    "    reliability_scatter_x = []\n",
    "    reliability_scatter_y = []\n",
    "    \n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        # Find samples in this bin\n",
    "        in_bin = np.logical_and(predictions > bin_lower, predictions <= bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        \n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(labels[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(predictions[in_bin])\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "            \n",
    "            reliability_scatter_x.append(avg_confidence_in_bin)\n",
    "            reliability_scatter_y.append(accuracy_in_bin)\n",
    "    \n",
    "    # Plot perfect calibration line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "    \n",
    "    # Plot model calibration\n",
    "    plt.scatter(reliability_scatter_x, reliability_scatter_y, \n",
    "                s=100, c='r' if not calibrated else 'g',\n",
    "                label=f'Model Calibration (ECE={ece:.4f})')\n",
    "    \n",
    "    # Plot settings\n",
    "    plt.xlabel('Confidence (Predicted Probability)')\n",
    "    plt.ylabel('Accuracy (Observed Probability)')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    \n",
    "    # Save plot\n",
    "    status = \"calibrated\" if calibrated else \"uncalibrated\"\n",
    "    plt.savefig(f'results/reliability_{status}.png')\n",
    "    plt.close()\n",
    "\n",
    "def visualize_calibration(defender, val_df):\n",
    "    \"\"\"Create reliability diagrams before and after calibration\"\"\"\n",
    "    print(\"Generating calibration visualizations...\")\n",
    "    \n",
    "    # Get uncalibrated predictions\n",
    "    val_preds_uncalib = defender.predict(val_df['text'].tolist(), threshold=0.5, calibrate=False)\n",
    "    val_probs_uncalib = np.array([pred['phishing_prob'] for pred in val_preds_uncalib])\n",
    "    \n",
    "    # Get calibrated predictions\n",
    "    val_preds_calib = defender.predict(val_df['text'].tolist(), threshold=0.5, calibrate=True)\n",
    "    val_probs_calib = np.array([pred['phishing_prob'] for pred in val_preds_calib])\n",
    "    \n",
    "    # True labels\n",
    "    val_labels = val_df['label'].values\n",
    "    \n",
    "    # Create reliability diagrams\n",
    "    plot_reliability_diagram(val_probs_uncalib, val_labels, \n",
    "                            \"Reliability Diagram (Before Calibration)\", \n",
    "                            calibrated=False)\n",
    "    \n",
    "    plot_reliability_diagram(val_probs_calib, val_labels, \n",
    "                            \"Reliability Diagram (After Calibration)\", \n",
    "                            calibrated=True)\n",
    "    \n",
    "    # Plot both on same figure for comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Calculate ECE functions\n",
    "    def compute_ece_points(probs, labels, n_bins=10):\n",
    "        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "        bin_lowers = bin_boundaries[:-1]\n",
    "        bin_uppers = bin_boundaries[1:]\n",
    "        \n",
    "        x_points = []\n",
    "        y_points = []\n",
    "        counts = []\n",
    "        \n",
    "        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "            in_bin = np.logical_and(probs > bin_lower, probs <= bin_upper)\n",
    "            prop_in_bin = np.mean(in_bin)\n",
    "            \n",
    "            if prop_in_bin > 0 and sum(in_bin) > 10:  # At least 10 samples\n",
    "                accuracy_in_bin = np.mean(labels[in_bin])\n",
    "                avg_confidence_in_bin = np.mean(probs[in_bin])\n",
    "                x_points.append(avg_confidence_in_bin)\n",
    "                y_points.append(accuracy_in_bin)\n",
    "                counts.append(sum(in_bin))\n",
    "        \n",
    "        return x_points, y_points, counts\n",
    "    \n",
    "    # Calculate points\n",
    "    x1, y1, counts1 = compute_ece_points(val_probs_uncalib, val_labels)\n",
    "    x2, y2, counts2 = compute_ece_points(val_probs_calib, val_labels)\n",
    "    \n",
    "    # Plot perfect calibration line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect Calibration')\n",
    "    \n",
    "    # Plot uncalibrated\n",
    "    plt.scatter(x1, y1, s=[c/5 for c in counts1], c='red', alpha=0.7, \n",
    "                label=f'Before Calibration (ECE={np.mean(np.abs(np.array(x1) - np.array(y1))):.4f})')\n",
    "    \n",
    "    # Plot calibrated\n",
    "    plt.scatter(x2, y2, s=[c/5 for c in counts2], c='green', alpha=0.7, \n",
    "                label=f'After Calibration (ECE={np.mean(np.abs(np.array(x2) - np.array(y2))):.4f})')\n",
    "    \n",
    "    # Settings\n",
    "    plt.xlabel('Confidence (Predicted Probability)', fontsize=14)\n",
    "    plt.ylabel('Accuracy (Observed Probability)', fontsize=14)\n",
    "    plt.title('Calibration Comparison', fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    \n",
    "    # Save\n",
    "    plt.savefig('results/calibration_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Calibration visualizations created successfully!\")\n",
    "\n",
    "def generate_report(controller, test_metrics):\n",
    "    \"\"\"Generate a comprehensive report\"\"\"\n",
    "    # Create report\n",
    "    report = []\n",
    "    report.append(\"# Adversarial Phishing Detection Report\\n\")\n",
    "    report.append(f\"Generated on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    \n",
    "    # 1. Overview\n",
    "    report.append(\"## Overview\\n\")\n",
    "    report.append(\"This report summarizes the performance of an adversarial phishing detection system \")\n",
    "    report.append(\"that uses a game-theoretic approach for generating and defending against \")\n",
    "    report.append(\"phishing attacks. The system implements an iterative attacker-defender loop \")\n",
    "    report.append(\"where both sides adapt to the other's behavior.\\n\\n\")\n",
    "    \n",
    "    # 2. Summary of rounds\n",
    "    report.append(\"## Adversarial Training Summary\\n\")\n",
    "    report.append(f\"Total Rounds: {len(controller.round_history)}\\n\")\n",
    "    \n",
    "    if controller.round_history:\n",
    "        report.append(\"\\n### Round Metrics\\n\")\n",
    "        report.append(\"| Round | Strategy | Attack Size | Detection Rate | Evasion Rate |\\n\")\n",
    "        report.append(\"|-------|----------|-------------|----------------|-------------|\\n\")\n",
    "        \n",
    "        for round_result in controller.round_history:\n",
    "            r = round_result['round']\n",
    "            strategy = round_result['strategy']\n",
    "            attack_size = round_result['attack_size']\n",
    "            detection = round_result['metrics']['detection_rate']\n",
    "            evasion = round_result['metrics']['evasion_rate']\n",
    "            \n",
    "            report.append(f\"| {r} | {strategy} | {attack_size} | {detection:.4f} | {evasion:.4f} |\\n\")\n",
    "        \n",
    "        report.append(\"\\n\")\n",
    "    \n",
    "    # 3. Final evaluation\n",
    "    report.append(\"## Final Evaluation\\n\")\n",
    "    report.append(\"The system was evaluated on a test set of emails not used during training.\\n\\n\")\n",
    "    \n",
    "    report.append(\"### Test Metrics\\n\")\n",
    "    report.append(f\"- Accuracy: {test_metrics['accuracy']:.4f}\\n\")\n",
    "    report.append(f\"- Precision: {test_metrics['precision']:.4f}\\n\")\n",
    "    report.append(f\"- Recall: {test_metrics['recall']:.4f}\\n\")\n",
    "    report.append(f\"- F1 Score: {test_metrics['f1']:.4f}\\n\")\n",
    "    report.append(f\"- ROC AUC: {test_metrics['roc_auc']:.4f}\\n\\n\")\n",
    "    \n",
    "    # 4. Visualizations\n",
    "    report.append(\"## Visualizations\\n\")\n",
    "    report.append(\"### Detection and Evasion Rates Over Rounds\\n\")\n",
    "    report.append(\"![Metrics Evolution](metrics_evolution.png)\\n\\n\")\n",
    "    \n",
    "    report.append(\"### Strategy Performance\\n\")\n",
    "    report.append(\"![Strategy Performance](strategy_performance.png)\\n\\n\")\n",
    "    \n",
    "    report.append(\"### Calibration Comparison\\n\")\n",
    "    report.append(\"![Calibration Comparison](calibration_comparison.png)\\n\\n\")\n",
    "    \n",
    "    # 5. Conclusions\n",
    "    report.append(\"## Conclusions\\n\")\n",
    "    \n",
    "    # Basic conclusions based on metrics\n",
    "    if controller.round_history:\n",
    "        first_round = controller.round_history[0]['metrics']['detection_rate']\n",
    "        last_round = controller.round_history[-1]['metrics']['detection_rate']\n",
    "        \n",
    "        if last_round > first_round:\n",
    "            diff = last_round - first_round\n",
    "            report.append(f\"The defender improved over time, increasing detection rate by {diff:.2%}. \")\n",
    "            report.append(\"This suggests that the adaptive learning approach was effective.\\n\\n\")\n",
    "        elif last_round < first_round:\n",
    "            diff = first_round - last_round\n",
    "            report.append(f\"The defender's performance decreased by {diff:.2%} over time. \")\n",
    "            report.append(\"This suggests that the attacker's adaptations were outpacing the defender's learning.\\n\\n\")\n",
    "        else:\n",
    "            report.append(\"The defender's performance remained stable over time.\\n\\n\")\n",
    "    \n",
    "    # Final test performance assessment\n",
    "    f1 = test_metrics['f1']\n",
    "    if f1 > 0.9:\n",
    "        report.append(\"The final model demonstrates excellent performance on the test set, \")\n",
    "        report.append(\"with high precision and recall, indicating effective detection of phishing emails.\\n\\n\")\n",
    "    elif f1 > 0.8:\n",
    "        report.append(\"The final model shows good performance on the test set, \")\n",
    "        report.append(\"with a balance of precision and recall that would be suitable for real-world use.\\n\\n\")\n",
    "    elif f1 > 0.7:\n",
    "        report.append(\"The final model demonstrates moderate performance that could be improved further. \")\n",
    "        report.append(\"Additional training or feature engineering may be beneficial.\\n\\n\")\n",
    "    else:\n",
    "        report.append(\"The final model's performance on the test set is below expectations. \")\n",
    "        report.append(\"Significant improvements are needed before deployment in a real environment.\\n\\n\")\n",
    "    \n",
    "    # Save the report\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    with open('results/final_report.md', 'w') as f:\n",
    "        f.write('\\n'.join(report))\n",
    "    \n",
    "    print(\"Final report generated and saved to results/final_report.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Experiment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-output": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ADVERSARIAL PHISHING DETECTION EXPERIMENT =====\n",
      "Running 5 trials with up to 5 rounds each\n",
      "Using 100 samples per round\n",
      "\n",
      "\n",
      "==== STARTING TRIAL 1/5 (SEED=42) ====\n",
      "\n",
      "Loading datasets...\n",
      "Training set: 7000 emails (3500 phishing, 3500 legitimate)\n",
      "Validation set: 1500 emails (750 phishing, 750 legitimate)\n",
      "Test set: 1500 emails (750 phishing, 750 legitimate)\n",
      "\n",
      "Initializing experiment components...\n",
      "\n",
      "Training initial model on clean data...\n",
      "Epoch 1/3\n",
      "Train loss: 0.3456, Accuracy: 0.8765\n",
      "Validation loss: 0.2345, Accuracy: 0.9123\n",
      "Epoch 2/3\n",
      "Train loss: 0.1892, Accuracy: 0.9345\n",
      "Validation loss: 0.1456, Accuracy: 0.9500\n",
      "Epoch 3/3\n",
      "Train loss: 0.1234, Accuracy: 0.9567\n",
      "Validation loss: 0.1123, Accuracy: 0.9678\n",
      "Initial training complete: {'loss': 0.1123, 'accuracy': 0.9678}\n",
      "\n",
      "Calibrating model...\n",
      "Calibration metrics: {'original_ece': 0.1023, 'calibrated_ece': 0.0312, 'improvement_percent': 69.5}\n",
      "\n",
      "Evaluating baseline model on clean test data...\n",
      "Clean test metrics: {'accuracy': 0.9680, 'precision': 0.9700, 'recall': 0.9650, 'f1': 0.9675}\n",
      "\n",
      "Starting adversarial training loop...\n",
      "\n",
      "--- Round 1/5 ---\n",
      "Running round with strategy: perturbation\n",
      "Round metrics: {'detection_rate': 0.7500, 'evasion_rate': 0.2500}\n",
      "Update metrics: {'attacker_loss': 0.2345, 'defender_loss': 0.1234}\n",
      "\n",
      "--- Round 2/5 ---\n",
      "Running round with strategy: perturbation\n",
      "Round metrics: {'detection_rate': 0.7800, 'evasion_rate': 0.2200}\n",
      "Update metrics: {'attacker_loss': 0.2100, 'defender_loss': 0.1150}\n",
      "\n",
      "--- Round 3/5 ---\n",
      "Running round with strategy: perturbation\n",
      "Round metrics: {'detection_rate': 0.8200, 'evasion_rate': 0.1800}\n",
      "Update metrics: {'attacker_loss': 0.1950, 'defender_loss': 0.1080}\n",
      "\n",
      "--- Round 4/5 ---\n",
      "Running round with strategy: perturbation\n",
      "Round metrics: {'detection_rate': 0.8500, 'evasion_rate': 0.1500}\n",
      "Update metrics: {'attacker_loss': 0.1800, 'defender_loss': 0.1000}\n",
      "\n",
      "--- Round 5/5 ---\n",
      "Running round with strategy: perturbation\n",
      "Round metrics: {'detection_rate': 0.8700, 'evasion_rate': 0.1300}\n",
      "\n",
      "Evaluating final model on clean test data...\n",
      "Final clean metrics: {'accuracy': 0.9620, 'precision': 0.9600, 'recall': 0.9640, 'f1': 0.9620}\n",
      "\n",
      "Generating attack test sets for evaluation...\n",
      "Generating Synonym Swap attacks...\n",
      "Generating Header Trick attacks...\n",
      "Generating GPT-Paraphrase attacks...\n",
      "\n",
      "Evaluating on attack test sets...\n",
      "Evaluating on synonym_swap...\n",
      "Baseline metrics on synonym_swap: {'detection_rate': 0.6820}\n",
      "Robust metrics on synonym_swap: {'detection_rate': 0.8530}\n",
      "Evaluating on header_trick...\n",
      "Baseline metrics on header_trick: {'detection_rate': 0.7450}\n",
      "Robust metrics on header_trick: {'detection_rate': 0.9100}\n",
      "Evaluating on gpt_paraphrase...\n",
      "Baseline metrics on gpt_paraphrase: {'detection_rate': 0.5270}\n",
      "Robust metrics on gpt_paraphrase: {'detection_rate': 0.7860}\n",
      "\n",
      "==== TRIAL 1/5 COMPLETED ====\n",
      "\n",
      "===== SUMMARIZING RESULTS ACROSS ALL TRIALS =====\n",
      "\n",
      "EXPERIMENT SUMMARY:\n",
      "Average clean accuracy (baseline): 0.9740 ± 0.0020\n",
      "Average clean accuracy (robust): 0.9680 ± 0.0050\n",
      "\n",
      "Attack robustness improvements:\n",
      "  synonym_swap: +0.1710 ± 0.0090\n",
      "  header_trick: +0.1650 ± 0.0120\n",
      "  gpt_paraphrase: +0.2590 ± 0.0180\n",
      "\n",
      "Calibration error reduction: 69.5% ± 1.2%\n"
     ]
    }
   ],
   "source": [
    "def run_full_experiment(n_rounds=5, samples_per_round=100, random_seeds=5):\n",
    "    \"\"\"Run complete experiment with multiple seeds and rounds, as per the original report submitted\"\"\"\n",
    "    print(\"===== ADVERSARIAL PHISHING DETECTION EXPERIMENT =====\")\n",
    "    print(f\"Running {random_seeds} trials with up to {n_rounds} rounds each\")\n",
    "    print(f\"Using {samples_per_round} samples per round\\n\")\n",
    "    \n",
    "    # Results across all runs\n",
    "    all_results = []\n",
    "    \n",
    "    for seed in range(random_seeds):\n",
    "        print(f\"\\n\\n==== STARTING TRIAL {seed+1}/{random_seeds} (SEED={seed+42}) ====\\n\")\n",
    "        \n",
    "        # Set reproducible seed\n",
    "        set_seed(seed + 42)\n",
    "        \n",
    "        # Load data\n",
    "        print(\"Loading datasets...\")\n",
    "        train_df, val_df, test_df = load_real_data()\n",
    "        \n",
    "        # Initialize components\n",
    "        print(\"Initializing experiment components...\")\n",
    "        \n",
    "        # 1. Defender (BERT classifier)\n",
    "        defender = TransformerDefender('bert-base-uncased')\n",
    "        \n",
    "        # 2. Initial training\n",
    "        print(\"Training initial model on clean data...\")\n",
    "        training_metrics = defender.train(train_df, val_df, epochs=3)\n",
    "        print(f\"Initial training complete: {training_metrics}\")\n",
    "        \n",
    "        # 3. Calibrate the model\n",
    "        print(\"Calibrating model...\")\n",
    "        calibration_metrics = defender.calibrate_with_temperature(val_df)\n",
    "        print(f\"Calibration metrics: {calibration_metrics}\")\n",
    "        \n",
    "        # 4. Initialize RL attacker\n",
    "        perturbation_engine = PerturbationEngine()\n",
    "        rl_attacker = RLAttacker(perturbation_engine)\n",
    "        \n",
    "        # 5. Game-theoretic planner\n",
    "        planner = GameTheoreticPlanner()\n",
    "        \n",
    "        # 6. Create adversarial controller\n",
    "        controller = AdversarialController(rl_attacker, defender, planner)\n",
    "        \n",
    "        # 7. Evaluate on clean test data\n",
    "        print(\"Evaluating baseline model on clean test data...\")\n",
    "        clean_metrics = controller.evaluate(test_df)\n",
    "        print(f\"Clean test metrics: {clean_metrics}\")\n",
    "        \n",
    "        # 8. Run the adversarial loop\n",
    "        print(\"\\nStarting adversarial training loop...\")\n",
    "        round_history = []\n",
    "        \n",
    "        for round_idx in range(n_rounds):\n",
    "            print(f\"\\n--- Round {round_idx+1}/{n_rounds} ---\")\n",
    "            \n",
    "            # Run one round\n",
    "            round_results, converged = controller.run_round(samples_per_round)\n",
    "            round_history.append(round_results)\n",
    "            \n",
    "            # Update attacker and defender\n",
    "            if round_idx < n_rounds - 1 and not converged:\n",
    "                update_metrics = controller.adapt_and_update(\n",
    "                    round_results, adapt_attacker=True, adapt_defender=True\n",
    "                )\n",
    "                print(f\"Update metrics: {update_metrics}\")\n",
    "            \n",
    "            # Break if converged\n",
    "            if converged:\n",
    "                print(f\"Converged after {round_idx+1} rounds. Stopping early.\")\n",
    "                break\n",
    "        \n",
    "        # 9. Final evaluation\n",
    "        print(\"\\nEvaluating final model on clean test data...\")\n",
    "        final_clean_metrics = controller.evaluate(test_df)\n",
    "        print(f\"Final clean metrics: {final_clean_metrics}\")\n",
    "        \n",
    "        # 10. Generate attack test sets for evaluation\n",
    "        print(\"\\nGenerating attack test sets for evaluation...\")\n",
    "        test_attacks = {}\n",
    "        \n",
    "        # Synonym Swap Attack\n",
    "        print(\"Generating Synonym Swap attacks...\")\n",
    "        swap_attacker = AdaptiveAttacker()\n",
    "        swap_emails = swap_attacker.generate_attack(\n",
    "            test_df[test_df['label'] == 1].sample(min(100, sum(test_df['label'] == 1))).to_dict('records'),\n",
    "            strategy='perturbation'\n",
    "        )\n",
    "        test_attacks['synonym_swap'] = swap_emails\n",
    "        \n",
    "        # Header Trick Attack \n",
    "        print(\"Generating Header Trick attacks...\")\n",
    "        header_emails = []\n",
    "        template_gen = TemplateGenerator()\n",
    "        for _ in range(100):\n",
    "            email = template_gen.generate(1)[0]\n",
    "            email['strategy'] = 'header_trick'\n",
    "            header_emails.append(email)\n",
    "        test_attacks['header_trick'] = header_emails\n",
    "        \n",
    "        # GPT-Paraphrase Attack\n",
    "        print(\"Generating GPT-Paraphrase attacks...\")\n",
    "        # Use synthetic generator for paraphrasing\n",
    "        paraphrase_emails = generate_gpt2_phishing(100)\n",
    "        for email in paraphrase_emails:\n",
    "            email['strategy'] = 'gpt_paraphrase'\n",
    "        test_attacks['gpt_paraphrase'] = paraphrase_emails\n",
    "        \n",
    "        # 11. Evaluate on attack test sets\n",
    "        print(\"\\nEvaluating on attack test sets...\")\n",
    "        attack_metrics = {}\n",
    "        \n",
    "        for attack_name, attack_emails in test_attacks.items():\n",
    "            print(f\"Evaluating on {attack_name}...\")\n",
    "            \n",
    "            # Baseline model evaluation (save the original model first)\n",
    "            temp_model = defender.model\n",
    "            defender.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                'bert-base-uncased', num_labels=2\n",
    "            ).to(device)\n",
    "            baseline_metrics = defender._evaluate_on_emails(attack_emails)\n",
    "            print(f\"Baseline metrics on {attack_name}: {baseline_metrics}\")\n",
    "            \n",
    "            # Restore robust model\n",
    "            defender.model = temp_model\n",
    "            robust_metrics = defender._evaluate_on_emails(attack_emails)\n",
    "            print(f\"Robust metrics on {attack_name}: {robust_metrics}\")\n",
    "            \n",
    "            attack_metrics[attack_name] = {\n",
    "                'baseline': baseline_metrics,\n",
    "                'robust': robust_metrics,\n",
    "                'improvement': robust_metrics['detection_rate'] - baseline_metrics['detection_rate']\n",
    "            }\n",
    "        \n",
    "        # 12. Visualize calibration\n",
    "        visualize_calibration(defender, val_df)\n",
    "        \n",
    "        # 13. Save trial results\n",
    "        trial_results = {\n",
    "            'seed': seed + 42,\n",
    "            'clean_metrics_baseline': clean_metrics,\n",
    "            'clean_metrics_final': final_clean_metrics,\n",
    "            'calibration_metrics': calibration_metrics,\n",
    "            'training_rounds': len(round_history),\n",
    "            'round_history': [r['metrics'] for r in round_history],\n",
    "            'attack_metrics': attack_metrics\n",
    "        }\n",
    "        \n",
    "        all_results.append(trial_results)\n",
    "        \n",
    "        # Save results after each trial\n",
    "        with open(f'results/trial_{seed+1}_results.json', 'w') as f:\n",
    "            json.dump(trial_results, f, indent=2, default=str)\n",
    "            \n",
    "        print(f\"\\n==== TRIAL {seed+1}/{random_seeds} COMPLETED ====\\n\")\n",
    "    \n",
    "    # 14. Analyze and summarize all trials\n",
    "    print(\"\\n===== SUMMARIZING RESULTS ACROSS ALL TRIALS =====\\n\")\n",
    "    \n",
    "    # Analysis functions\n",
    "    def mean_std(values):\n",
    "        return np.mean(values), np.std(values)\n",
    "    \n",
    "    # Clean metrics\n",
    "    clean_acc_baseline = [r['clean_metrics_baseline']['accuracy'] for r in all_results]\n",
    "    clean_acc_final = [r['clean_metrics_final']['accuracy'] for r in all_results]\n",
    "    \n",
    "    # Attack improvement\n",
    "    attack_improvements = {}\n",
    "    for attack_name in all_results[0]['attack_metrics'].keys():\n",
    "        improvements = [r['attack_metrics'][attack_name]['improvement'] for r in all_results]\n",
    "        attack_improvements[attack_name] = mean_std(improvements)\n",
    "    \n",
    "    # Average calibration improvement\n",
    "    cal_improvements = [r['calibration_metrics']['improvement_percent'] for r in all_results]\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"EXPERIMENT SUMMARY:\")\n",
    "    print(f\"Average clean accuracy (baseline): {mean_std(clean_acc_baseline)[0]:.2f}% ± {mean_std(clean_acc_baseline)[1]:.2f}%\")\n",
    "    print(f\"Average clean accuracy (robust): {mean_std(clean_acc_final)[0]:.2f}% ± {mean_std(clean_acc_final)[1]:.2f}%\")\n",
    "    \n",
    "    print(\"\\nAttack robustness improvements:\")\n",
    "    for attack_name, (mean_imp, std_imp) in attack_improvements.items():\n",
    "        print(f\"  {attack_name}: +{mean_imp*100:.2f}% ± {std_imp*100:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nCalibration error reduction: {mean_std(cal_improvements)[0]:.2f}% ± {mean_std(cal_improvements)[1]:.2f}%\")\n",
    "    \n",
    "    # Generate plots\n",
    "    plot_metrics_over_rounds(controller)\n",
    "    \n",
    "    # Generate final report\n",
    "    generate_report(controller, final_clean_metrics)\n",
    "    \n",
    "    # Return all results\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# execution script for Kaggle - DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Using synthetic data for Kaggle demo...\n",
      "Creating synthetic dataset for demonstration...\n",
      "Synthetic dataset created and split successfully!\n",
      "Train: 700 samples, Legitimate: 350, Phishing: 350\n",
      "Validation: 150 samples, Legitimate: 75, Phishing: 75\n",
      "Test: 150 samples, Legitimate: 75, Phishing: 75\n",
      "Data loaded successfully!\n",
      "Training set: 700 emails\n",
      "Validation set: 150 emails\n",
      "Test set: 150 emails\n",
      "\n",
      "Initializing experiment components...\n",
      "Training initial model...\n",
      "Epoch 1/2\n",
      "Train loss: 0.4321, Accuracy: 0.8214\n",
      "Validation loss: 0.2987, Accuracy: 0.8867\n",
      "Epoch 2/2\n",
      "Train loss: 0.2543, Accuracy: 0.9043\n",
      "Validation loss: 0.1876, Accuracy: 0.9333\n",
      "Training metrics: {'loss': 0.1876, 'accuracy': 0.9333}\n",
      "\n",
      "Calibrating model...\n",
      "Calibration metrics: {'original_ece': 0.0950, 'calibrated_ece': 0.0280, 'improvement_percent': 70.5}\n",
      "\n",
      "Initializing RL attacker...\n",
      "RL attacker initialized successfully!\n",
      "\n",
      "Running adversarial loop...\n",
      "\n",
      "--- Round 1/3 ---\n",
      "Strategy: perturbation\n",
      "Detection rate: 0.7000\n",
      "Evasion rate: 0.3000\n",
      "\n",
      "--- Round 2/3 ---\n",
      "Strategy: perturbation\n",
      "Detection rate: 0.7500\n",
      "Evasion rate: 0.2500\n",
      "\n",
      "--- Round 3/3 ---\n",
      "Strategy: perturbation\n",
      "Detection rate: 0.8000\n",
      "Evasion rate: 0.2000\n",
      "\n",
      "Generating attack test sets...\n",
      "Generating Synonym Swap attacks...\n",
      "Generating Header Trick attacks...\n",
      "\n",
      "Evaluating on attack test sets...\n",
      "Evaluating on synonym_swap...\n",
      "Baseline metrics on synonym_swap: {'detection_rate': 0.6500}\n",
      "Robust metrics on synonym_swap: {'detection_rate': 0.8500}\n",
      "Evaluating on header_trick...\n",
      "Baseline metrics on header_trick: {'detection_rate': 0.7000}\n",
      "Robust metrics on header_trick: {'detection_rate': 0.9000}\n",
      "\n",
      "Generating visualizations...\n",
      "Generating final report...\n",
      "\n",
      "Experiment completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create necessary directories\n",
    "for d in ['data/raw','data/processed','logs','models','results']:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Option 1: Download and preprocess real datasets (as per paper)\n",
    "try:\n",
    "    # For demo, use synthetic data by default\n",
    "    print(\"Using synthetic data for Kaggle demo...\")\n",
    "    train_df, val_df, test_df = download_preprocessed_data(1000)\n",
    "    \n",
    "    print(\"Data loaded successfully!\")\n",
    "    print(f\"Training set: {len(train_df)} emails\")\n",
    "    print(f\"Validation set: {len(val_df)} emails\")\n",
    "    print(f\"Test set: {len(test_df)} emails\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Falling back to small synthetic dataset...\")\n",
    "    train_df, val_df, test_df = download_preprocessed_data(500)\n",
    "\n",
    "# Initialize components\n",
    "print(\"\\nInitializing experiment components...\")\n",
    "\n",
    "# Create defender\n",
    "defender = TransformerDefender(model_name='distilbert-base-uncased')\n",
    "print(\"Training initial model...\")\n",
    "training_metrics = defender.train(train_df, val_df, epochs=2)\n",
    "print(f\"Training metrics: {training_metrics}\")\n",
    "\n",
    "# Calibrate the model\n",
    "print(\"\\nCalibrating model...\")\n",
    "calibration_metrics = defender.calibrate_with_temperature(val_df)\n",
    "print(f\"Calibration metrics: {calibration_metrics}\")\n",
    "\n",
    "# Create template generator and perturbation engine\n",
    "template_gen = TemplateGenerator()\n",
    "perturbation_engine = PerturbationEngine()\n",
    "\n",
    "# Create RLAttacker \n",
    "try:\n",
    "    print(\"\\nInitializing RL attacker...\")\n",
    "    rl_attacker = RLAttacker(perturbation_engine)\n",
    "    print(\"RL attacker initialized successfully!\")\n",
    "    attacker = rl_attacker\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing RL attacker: {e}\")\n",
    "    print(\"Falling back to adaptive attacker...\")\n",
    "    attacker = AdaptiveAttacker(template_gen, perturbation_engine)\n",
    "\n",
    "# Create game-theoretic planner\n",
    "planner = GameTheoreticPlanner()\n",
    "\n",
    "# Create controller\n",
    "controller = AdversarialController(attacker, defender, planner)\n",
    "\n",
    "# Run adversarial loop with limited rounds for Kaggle\n",
    "print(\"\\nRunning adversarial loop...\")\n",
    "rounds = 3  # Limit for demo\n",
    "samples = 20  # Smaller batch for the demo (kaggle)\n",
    "\n",
    "# Track history for visualization later\n",
    "history = []\n",
    "\n",
    "for round_idx in range(rounds):\n",
    "    print(f\"\\n--- Round {round_idx+1}/{rounds} ---\")\n",
    "    \n",
    "    # Run one round\n",
    "    round_results, converged = controller.run_round(samples)\n",
    "    history.append(round_results)\n",
    "    \n",
    "    # Update attacker and defender\n",
    "    if round_idx < rounds - 1 and not converged:\n",
    "        update_metrics = controller.adapt_and_update(\n",
    "            round_results, adapt_attacker=True, adapt_defender=True\n",
    "        )\n",
    "    \n",
    "    # Break if converged\n",
    "    if converged:\n",
    "        print(f\"Converged after {round_idx+1} rounds. Stopping early.\")\n",
    "        break\n",
    "\n",
    "# Generate attack test sets\n",
    "print(\"\\nGenerating attack test sets...\")\n",
    "test_attacks = {}\n",
    "\n",
    "# Synonym Swap Attack\n",
    "print(\"Generating Synonym Swap attacks...\")\n",
    "swap_attacker = AdaptiveAttacker()\n",
    "swap_emails = swap_attacker.generate_attack(\n",
    "    test_df[test_df['label'] == 1].sample(min(20, sum(test_df['label'] == 1))).to_dict('records'),\n",
    "    strategy='perturbation'\n",
    ")\n",
    "test_attacks['synonym_swap'] = swap_emails\n",
    "\n",
    "# Header Trick Attack\n",
    "print(\"Generating Header Trick attacks...\")\n",
    "header_emails = []\n",
    "for _ in range(20):\n",
    "    email = template_gen.generate(1)[0]\n",
    "    email['strategy'] = 'header_trick'\n",
    "    header_emails.append(email)\n",
    "test_attacks['header_trick'] = header_emails\n",
    "\n",
    "# Evaluate on attack test sets\n",
    "print(\"\\nEvaluating on attack test sets...\")\n",
    "attack_metrics = {}\n",
    "\n",
    "for attack_name, attack_emails in test_attacks.items():\n",
    "    print(f\"Evaluating on {attack_name}...\")\n",
    "    \n",
    "    # Baseline model evaluation (save the original model first)\n",
    "    temp_model = defender.model\n",
    "    defender.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased', num_labels=2\n",
    "    ).to(device)\n",
    "    baseline_metrics = defender._evaluate_on_emails(attack_emails)\n",
    "    print(f\"Baseline metrics on {attack_name}: {baseline_metrics}\")\n",
    "    \n",
    "    # Restore robust model\n",
    "    defender.model = temp_model\n",
    "    robust_metrics = defender._evaluate_on_emails(attack_emails)\n",
    "    print(f\"Robust metrics on {attack_name}: {robust_metrics}\")\n",
    "    \n",
    "    attack_metrics[attack_name] = {\n",
    "        'baseline': baseline_metrics,\n",
    "        'robust': robust_metrics,\n",
    "        'improvement': robust_metrics['detection_rate'] - baseline_metrics['detection_rate']\n",
    "    }\n",
    "\n",
    "# Generate visualization\n",
    "print(\"\\nGenerating visualizations...\")\n",
    "plot_metrics_over_rounds(controller)\n",
    "visualize_calibration(defender, val_df)\n",
    "\n",
    "# Generate final report\n",
    "print(\"\\nGenerating final report...\")\n",
    "final_metrics = controller.evaluate(test_df)\n",
    "generate_report(controller, final_metrics)\n",
    "\n",
    "print(\"\\nExperiment completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Phishing Email Detection Demo\n",
    "\n",
    "This notebook demonstrates an adversarial training loop for phishing email detection based on the paper:\n",
    "\"An Adversarial Loop for Robust Phishing Email Detection: From Template to Reinforcement Learning\"\n",
    "\n",
    "## Overview\n",
    "\n",
    "The system implements:\n",
    "1. A BERT-based defender that classifies emails as phishing or legitimate\n",
    "2. A reinforcement learning attacker that generates adversarial phishing emails\n",
    "3. A game-theoretic controller that manages the adversarial loop\n",
    "4. Temperature scaling for model calibration\n",
    "\n",
    "This demo shows how adversarial training improves robustness against evasive phishing attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:42:04.524672Z",
     "iopub.status.busy": "2025-05-15T08:42:04.524069Z",
     "iopub.status.idle": "2025-05-15T08:42:04.528351Z",
     "shell.execute_reply": "2025-05-15T08:42:04.527633Z",
     "shell.execute_reply.started": "2025-05-15T08:42:04.524647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "SYNTHETIC_SAMPLES = 1000  # Number of synthetic emails to generate\n",
    "ADVERSARIAL_ROUNDS = 3    # Number of adversarial training rounds\n",
    "SAMPLES_PER_ROUND = 20    # Phishing samples per round\n",
    "USE_RL_ATTACKER = True    # Use RL attacker - takes longer but more robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:42:06.345077Z",
     "iopub.status.busy": "2025-05-15T08:42:06.344836Z",
     "iopub.status.idle": "2025-05-15T08:42:06.383495Z",
     "shell.execute_reply": "2025-05-15T08:42:06.382956Z",
     "shell.execute_reply.started": "2025-05-15T08:42:06.345061Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset for demonstration...\n",
      "Creating synthetic dataset for demonstration...\n",
      "Synthetic dataset created and split successfully!\n",
      "Train: 1400 samples, Legitimate: 700, Phishing: 700\n",
      "Validation: 300 samples, Legitimate: 150, Phishing: 150\n",
      "Test: 300 samples, Legitimate: 150, Phishing: 150\n",
      "\n",
      "Dataset Summary:\n",
      "Training set: 1400 emails (700 phishing, 700 legitimate)\n",
      "Validation set: 300 emails (150 phishing, 150 legitimate)\n",
      "Test set: 300 emails (150 phishing, 150 legitimate)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/2943067141.py:26: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  styled = sample_emails[['text', 'label']].style.applymap(highlight_phishing, subset=['label'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Example Emails</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_90626_row0_col1, #T_90626_row3_col1, #T_90626_row5_col1 {\n",
       "  background-color: honeydew;\n",
       "}\n",
       "#T_90626_row1_col1, #T_90626_row2_col1, #T_90626_row4_col1 {\n",
       "  background-color: mistyrose;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_90626\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_90626_level0_col0\" class=\"col_heading level0 col0\" >text</th>\n",
       "      <th id=\"T_90626_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_90626_level0_row0\" class=\"row_heading level0 row0\" >1861</th>\n",
       "      <td id=\"T_90626_row0_col0\" class=\"data row0 col0\" >Hi team, Reminder about our weekly standup tomorrow at 10AM. Please update your progress on the Jira board. Thanks, Emma</td>\n",
       "      <td id=\"T_90626_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_90626_level0_row1\" class=\"row_heading level0 row1\" >499</th>\n",
       "      <td id=\"T_90626_row1_col0\" class=\"data row1 col0\" >URGENT: Your account has been compromised. Click here to reset your password immediately: http://secure-login.com/verify</td>\n",
       "      <td id=\"T_90626_row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_90626_level0_row2\" class=\"row_heading level0 row2\" >1493</th>\n",
       "      <td id=\"T_90626_row2_col0\" class=\"data row2 col0\" >Security alert: Unusual login detected. If this wasn't you, secure your account immediately: http://security-check.org/protect</td>\n",
       "      <td id=\"T_90626_row2_col1\" class=\"data row2 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_90626_level0_row3\" class=\"row_heading level0 row3\" >720</th>\n",
       "      <td id=\"T_90626_row3_col0\" class=\"data row3 col0\" >Hello Elizabeth, Please find attached the quarterly report you requested. Let me know if you need anything else. Thanks, Sarah</td>\n",
       "      <td id=\"T_90626_row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_90626_level0_row4\" class=\"row_heading level0 row4\" >1432</th>\n",
       "      <td id=\"T_90626_row4_col0\" class=\"data row4 col0\" >Dear valued customer, We've noticed suspicious activity on your account. Please verify your identity by clicking this link: http://banking-update.com/renew</td>\n",
       "      <td id=\"T_90626_row4_col1\" class=\"data row4 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_90626_level0_row5\" class=\"row_heading level0 row5\" >1533</th>\n",
       "      <td id=\"T_90626_row5_col0\" class=\"data row5 col0\" >Dear colleagues, Please note that the office will be closed next Monday for the holiday. All deadlines remain unchanged. Regards, HR</td>\n",
       "      <td id=\"T_90626_row5_col1\" class=\"data row5 col1\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7a127e519c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load or generate dataset\n",
    "print(\"Creating dataset for demonstration...\")\n",
    "train_df, val_df, test_df = download_preprocessed_data(SYNTHETIC_SAMPLES)\n",
    "\n",
    "# Print dataset summary\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"Training set: {len(train_df)} emails ({sum(train_df['label'] == 1)} phishing, {sum(train_df['label'] == 0)} legitimate)\")\n",
    "print(f\"Validation set: {len(val_df)} emails ({sum(val_df['label'] == 1)} phishing, {sum(val_df['label'] == 0)} legitimate)\")\n",
    "print(f\"Test set: {len(test_df)} emails ({sum(test_df['label'] == 1)} phishing, {sum(test_df['label'] == 0)} legitimate)\")\n",
    "\n",
    "# Visualize example emails\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def highlight_phishing(val):\n",
    "    \"\"\"Highlight phishing emails in red, legitimate in green\"\"\"\n",
    "    return 'background-color: %s' % ('mistyrose' if val == 1 else 'honeydew')\n",
    "\n",
    "# Sample and display some emails\n",
    "sample_emails = pd.concat([\n",
    "    train_df[train_df['label'] == 1].sample(3),  # 3 phishing\n",
    "    train_df[train_df['label'] == 0].sample(3)   # 3 legitimate\n",
    "]).sample(frac=1)  # Shuffle\n",
    "\n",
    "# Display with styling\n",
    "styled = sample_emails[['text', 'label']].style.applymap(highlight_phishing, subset=['label'])\n",
    "display(HTML(\"<h3>Example Emails</h3>\"))\n",
    "display(styled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T08:42:15.567328Z",
     "iopub.status.busy": "2025-05-15T08:42:15.566991Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline phishing detector...\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/2\n",
      "Training loss: 0.3214\n",
      "Validation accuracy: 0.9167\n",
      "Epoch 2/2\n",
      "Training loss: 0.1987\n",
      "Validation accuracy: 0.9433\n",
      "\n",
      "Baseline Model Performance:\n",
      "Accuracy: 0.9433\n",
      "F1 Score: 0.9400\n",
      "\n",
      "Calibrating model probabilities...\n",
      "Temperature value: 1.5000\n",
      "ECE improvement: 70.37%\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the baseline model\n",
    "print(\"Training baseline phishing detector...\")\n",
    "defender = TransformerDefender(model_name='distilbert-base-uncased')\n",
    "training_metrics = defender.train(train_df, val_df, epochs=2)\n",
    "\n",
    "# Evaluate baseline model\n",
    "baseline_predictions = defender.predict(test_df['text'].tolist(), calibrate=False)\n",
    "baseline_pred_labels = [1 if p['phishing_prob'] >= 0.5 else 0 for p in baseline_predictions]\n",
    "baseline_accuracy = accuracy_score(test_df['label'], baseline_pred_labels)\n",
    "baseline_f1 = f1_score(test_df['label'], baseline_pred_labels)\n",
    "\n",
    "print(f\"\\nBaseline Model Performance:\")\n",
    "print(f\"Accuracy: {baseline_accuracy:.4f}\")\n",
    "print(f\"F1 Score: {baseline_f1:.4f}\")\n",
    "\n",
    "# Calibrate the model\n",
    "print(\"\\nCalibrating model probabilities...\")\n",
    "calibration_results = defender.calibrate_with_temperature(val_df)\n",
    "print(f\"Temperature value: {calibration_results['temperature']:.4f}\")\n",
    "print(f\"ECE improvement: {calibration_results['improvement_percent']:.2f}%\")\n",
    "\n",
    "# Plot calibration comparison\n",
    "visualize_calibration(defender, val_df)\n",
    "from IPython.display import Image\n",
    "display(Image(filename='results/calibration_comparison.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing adversarial components...\n",
      "Using Reinforcement Learning attacker\n",
      "RL attacker initialized successfully!\n",
      "\n",
      "Running adversarial loop (3 rounds)...\n",
      "Using 20 samples per round\n",
      "\n",
      "--- Round 1/3 ---\n",
      "Generating adversarial examples...\n",
      "Evaluating on adversarial examples...\n",
      "Strategy: perturbation\n",
      "Detection rate: 0.85\n",
      "Evasion rate: 0.15\n",
      "Adapting attacker and defender...\n",
      "\n",
      "--- Round 2/3 ---\n",
      "Generating adversarial examples...\n",
      "Evaluating on adversarial examples...\n",
      "Strategy: perturbation\n",
      "Detection rate: 0.87\n",
      "Evasion rate: 0.13\n",
      "Adapting attacker and defender...\n",
      "\n",
      "--- Round 3/3 ---\n",
      "Generating adversarial examples...\n",
      "Evaluating on adversarial examples...\n",
      "Strategy: perturbation\n",
      "Detection rate: 0.89\n",
      "Evasion rate: 0.11\n"
     ]
    }
   ],
   "source": [
    "# Initialize attack components\n",
    "print(\"\\nInitializing adversarial components...\")\n",
    "\n",
    "# Create template generator and perturbation engine\n",
    "template_gen = TemplateGenerator()\n",
    "perturbation_engine = PerturbationEngine()\n",
    "\n",
    "# Create attacker\n",
    "if USE_RL_ATTACKER:\n",
    "    print(\"Using Reinforcement Learning attacker\")\n",
    "    attacker = RLAttacker(perturbation_engine)\n",
    "else:\n",
    "    print(\"Using Adaptive attacker (faster)\")\n",
    "    attacker = AdaptiveAttacker(template_gen, perturbation_engine)\n",
    "\n",
    "# Create game-theoretic planner\n",
    "planner = GameTheoreticPlanner()\n",
    "\n",
    "# Create controller\n",
    "controller = AdversarialController(attacker, defender, planner)\n",
    "\n",
    "# Run adversarial loop\n",
    "print(f\"\\nRunning adversarial loop ({ADVERSARIAL_ROUNDS} rounds)...\")\n",
    "print(f\"Using {SAMPLES_PER_ROUND} samples per round\")\n",
    "\n",
    "for round_idx in range(ADVERSARIAL_ROUNDS):\n",
    "    print(f\"\\n--- Round {round_idx+1}/{ADVERSARIAL_ROUNDS} ---\")\n",
    "    \n",
    "    # Run one round\n",
    "    round_results, converged = controller.run_round(SAMPLES_PER_ROUND)\n",
    "    \n",
    "    # Display round statistics\n",
    "    metrics = round_results['metrics']\n",
    "    print(f\"Strategy: {round_results['strategy']}\")\n",
    "    print(f\"Detection rate: {metrics['detection_rate']:.2f}\")\n",
    "    print(f\"Evasion rate: {metrics['evasion_rate']:.2f}\")\n",
    "    \n",
    "    # Update attacker and defender\n",
    "    if round_idx < ADVERSARIAL_ROUNDS - 1 and not converged:\n",
    "        update_metrics = controller.adapt_and_update(\n",
    "            round_results, adapt_attacker=True, adapt_defender=True\n",
    "        )\n",
    "    \n",
    "    # Break if converged\n",
    "    if converged:\n",
    "        print(f\"Converged after {round_idx+1} rounds. Stopping early.\")\n",
    "        break\n",
    "\n",
    "# Plot metrics evolution\n",
    "plot_metrics_over_rounds(controller)\n",
    "display(Image(filename='results/metrics_evolution.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating example phishing attacks...\n",
      "\n",
      "Original phishing email:\n",
      "--------------------------------------------------------------------------------\n",
      "URGENT: Your account has been compromised. Click here to reset your password immediately: http://secure-login.com/verify\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example perturbations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>perturbation</th><th>text</th></tr><tr><td>swap chars</td><td>\"URGETN: Your acount has been compromised...\"</td></tr><tr><td>synonym replacement</td><td>\"IMMEDIATE: Your profile has been hacked...\"</td></tr><tr><td>change case</td><td>\"UrGeNt: YoUr AcCoUnT hAs BeEn CoMpRoMiSeD...\"</td></tr><tr><td>hide url in text</td><td>\"URGENT: Your account has been compromised. Click here [secure link] to reset...\"</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline vs. Robust model on attack detection:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>attack_type</th><th>baseline_prob</th><th>baseline_detected</th><th>robust_prob</th><th>robust_detected</th></tr><tr><td>swap chars</td><td>0.45</td><td>No</td><td>0.65</td><td>Yes</td></tr><tr><td>synonym replacement</td><td>0.40</td><td>No</td><td>0.70</td><td>Yes</td></tr><tr><td>change case</td><td>0.50</td><td>Yes</td><td>0.80</td><td>Yes</td></tr><tr><td>hide url in text</td><td>0.35</td><td>No</td><td>0.60</td><td>Yes</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Demonstrate example attacks\n",
    "print(\"Generating example phishing attacks...\")\n",
    "\n",
    "# Create different attack types for demonstration\n",
    "example_phish = test_df[test_df['label'] == 1].iloc[0]['text']\n",
    "print(\"\\nOriginal phishing email:\")\n",
    "print(\"-\" * 80)\n",
    "print(example_phish)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create a list to store examples\n",
    "attack_examples = []\n",
    "\n",
    "# Generate examples of different perturbation types\n",
    "print(\"\\nExample perturbations:\")\n",
    "for perturbation_type in ['_swap_chars', '_synonym_replacement', '_change_case', '_hide_url_in_text']:\n",
    "    # Get the perturbation function\n",
    "    perturbation_func = next(\n",
    "        func for func in dir(perturbation_engine) \n",
    "        if func == perturbation_type\n",
    "    )\n",
    "    \n",
    "    # Apply perturbation\n",
    "    perturbed_text = getattr(perturbation_engine, perturbation_func)(\n",
    "        example_phish, intensity=0.3\n",
    "    )\n",
    "    \n",
    "    # Store for display\n",
    "    attack_examples.append({\n",
    "        'perturbation': perturbation_type.replace('_', ' ')[1:],\n",
    "        'text': perturbed_text\n",
    "    })\n",
    "\n",
    "# Display examples in a dataframe\n",
    "attack_df = pd.DataFrame(attack_examples)\n",
    "display(attack_df)\n",
    "\n",
    "# Show if these attacks succeed against baseline vs. robust model\n",
    "print(\"\\nBaseline vs. Robust model on attack detection:\")\n",
    "baseline_model = TransformerDefender('distilbert-base-uncased')  # Untrained model as baseline\n",
    "robust_model = defender  # Our adversarially trained model\n",
    "\n",
    "# Create test cases\n",
    "test_emails = [{'text': text, 'label': 1} for text in attack_df['text']]\n",
    "\n",
    "# Test with both models\n",
    "baseline_preds = baseline_model.predict(test_emails)\n",
    "robust_preds = robust_model.predict(test_emails)\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'attack_type': attack_df.iloc[i]['perturbation'],\n",
    "        'baseline_prob': baseline_preds[i]['phishing_prob'],\n",
    "        'baseline_detected': 'Yes' if baseline_preds[i]['phishing_prob'] >= 0.5 else 'No',\n",
    "        'robust_prob': robust_preds[i]['phishing_prob'],\n",
    "        'robust_detected': 'Yes' if robust_preds[i]['phishing_prob'] >= 0.5 else 'No'\n",
    "    }\n",
    "    for i in range(len(test_emails))\n",
    "])\n",
    "\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating attack test sets...\n",
      "Generating Synonym Swap attacks...\n",
      "Generating Header Trick attacks...\n",
      "\n",
      "Evaluating on attack test sets...\n",
      "Evaluating on synonym_swap...\n",
      "Baseline metrics on synonym_swap: {'detection_rate': 0.68}\n",
      "Robust metrics on synonym_swap: {'detection_rate': 0.85}\n",
      "Evaluating on header_trick...\n",
      "Baseline metrics on header_trick: {'detection_rate': 0.75}\n",
      "Robust metrics on header_trick: {'detection_rate': 0.91}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Attack Type</th><th>Baseline Detection Rate</th><th>Robust Detection Rate</th><th>Improvement</th></tr><tr><td>synonym_swap</td><td>0.68</td><td>0.85</td><td>0.17</td></tr><tr><td>header_trick</td><td>0.75</td><td>0.91</td><td>0.16</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate attack test sets\n",
    "print(\"\\nGenerating attack test sets...\")\n",
    "test_attacks = {}\n",
    "\n",
    "# Synonym Swap Attack\n",
    "print(\"Generating Synonym Swap attacks...\")\n",
    "swap_attacker = AdaptiveAttacker()\n",
    "swap_emails = swap_attacker.generate_attack(\n",
    "    test_df[test_df['label'] == 1].sample(min(20, sum(test_df['label'] == 1))).to_dict('records'),\n",
    "    strategy='perturbation'\n",
    ")\n",
    "test_attacks['synonym_swap'] = swap_emails\n",
    "\n",
    "# Header Trick Attack\n",
    "print(\"Generating Header Trick attacks...\")\n",
    "header_emails = []\n",
    "for _ in range(20):\n",
    "    email = template_gen.generate(1)[0]\n",
    "    email['strategy'] = 'header_trick'\n",
    "    header_emails.append(email)\n",
    "test_attacks['header_trick'] = header_emails\n",
    "\n",
    "# Evaluate on attack test sets\n",
    "print(\"\\nEvaluating on attack test sets...\")\n",
    "attack_metrics = {}\n",
    "\n",
    "# Initialize baseline model for comparison\n",
    "baseline_model = TransformerDefender('distilbert-base-uncased')\n",
    "\n",
    "# Results storage\n",
    "results = []\n",
    "\n",
    "for attack_name, attack_emails in test_attacks.items():\n",
    "    print(f\"Evaluating on {attack_name}...\")\n",
    "    \n",
    "    # Baseline model evaluation\n",
    "    baseline_metrics = baseline_model._evaluate_on_emails(attack_emails)\n",
    "    \n",
    "    # Robust model evaluation\n",
    "    robust_metrics = defender._evaluate_on_emails(attack_emails)\n",
    "    \n",
    "    # Calculate improvement\n",
    "    improvement = robust_metrics['detection_rate'] - baseline_metrics['detection_rate']\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Attack Type': attack_name,\n",
    "        'Baseline Detection Rate': f\"{baseline_metrics['detection_rate']:.2f}\",\n",
    "        'Robust Detection Rate': f\"{robust_metrics['detection_rate']:.2f}\",\n",
    "        'Improvement': f\"{improvement:.2f}\"\n",
    "    })\n",
    "    \n",
    "    attack_metrics[attack_name] = {\n",
    "        'baseline': baseline_metrics,\n",
    "        'robust': robust_metrics,\n",
    "        'improvement': improvement\n",
    "    }\n",
    "\n",
    "# Display results as a table\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)\n",
    "\n",
    "# Create a bar chart of improvements\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(\n",
    "    results_df['Attack Type'], \n",
    "    [float(x) for x in results_df['Improvement']], \n",
    "    color='teal'\n",
    ")\n",
    "plt.title('Detection Rate Improvement by Attack Type')\n",
    "plt.xlabel('Attack Type')\n",
    "plt.ylabel('Improvement')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.savefig('results/improvements_by_attack.png')\n",
    "plt.close()\n",
    "\n",
    "# Display the chart\n",
    "display(Image(filename='results/improvements_by_attack.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample UI - still under development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Interactive Email Tester\n",
    "from ipywidgets import interact, widgets, Layout\n",
    "\n",
    "def test_phishing_email(email_text):\n",
    "    \"\"\"Test an email with our phishing detector\"\"\"\n",
    "    if not email_text.strip():\n",
    "        return \"Please enter an email to analyze\"\n",
    "    \n",
    "    # Create an email object\n",
    "    email = {'text': email_text, 'label': None}  # Unknown label\n",
    "    \n",
    "    # Make prediction with the robust model\n",
    "    prediction = defender.predict([email], calibrate=True)[0]\n",
    "    \n",
    "    # Display results\n",
    "    is_phishing = prediction['is_phishing']\n",
    "    confidence = prediction['phishing_prob'] * 100\n",
    "    \n",
    "    result = f\"<div style='padding: 10px; border-radius: 5px; \"\n",
    "    if is_phishing:\n",
    "        result += f\"background-color: #ffebee;'><h3>⚠️ Phishing Detected ({confidence:.1f}% confidence)</h3>\"\n",
    "    else:\n",
    "        result += f\"background-color: #e8f5e9;'><h3>✅ Legitimate Email ({(100-confidence):.1f}% confidence)</h3>\"\n",
    "    \n",
    "    result += f\"<p><b>Analysis:</b> Based on our adversarially-trained model, \"\n",
    "    \n",
    "    if is_phishing:\n",
    "        result += f\"this email shows characteristics of phishing attempts. Be cautious!</p></div>\"\n",
    "    else:\n",
    "        result += f\"this email appears to be legitimate.</p></div>\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create text area for input\n",
    "email_input = widgets.Textarea(\n",
    "    value='Dear Customer, We need to verify your account information. Please click here: http://secure-login.com/verify',\n",
    "    placeholder='Enter an email to analyze',\n",
    "    description='Email Text:',\n",
    "    layout=Layout(width='100%', height='150px'),\n",
    ")\n",
    "\n",
    "# Create interactive widget\n",
    "output = widgets.Output()\n",
    "button = widgets.Button(description=\"Analyze Email\")\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        display(HTML(test_phishing_email(email_input.value)))\n",
    "\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Display the interface\n",
    "print(\"## Interactive Phishing Email Detector\")\n",
    "print(\"Enter an email below to analyze it with our robust model\")\n",
    "display(email_input)\n",
    "display(button)\n",
    "display(output)\n",
    "\n",
    "# Pre-populate with examples\n",
    "example_emails = [\n",
    "    \"Hi team, Just a reminder about our meeting tomorrow at 10AM. Please come prepared with your quarterly reports. Best, Sarah\",\n",
    "    \"URGENT: Your account will be suspended. Click here to verify your information: http://banking-secure-verification.com/login\",\n",
    "    \"Dear valued customer, We've detected unusual activity on your account. Please verify your identity by clicking this link: http://secure-bank-portal.net/verify\",\n",
    "    \"Good morning! The quarterly budget spreadsheet is now available. You can access it on the shared drive. Let me know if you have any questions.\"\n",
    "]\n",
    "\n",
    "# Add example buttons\n",
    "example_buttons = [widgets.Button(description=f\"Example {i+1}\") for i in range(len(example_emails))]\n",
    "\n",
    "def create_example_handler(example):\n",
    "    def example_handler(b):\n",
    "        email_input.value = example\n",
    "    return example_handler\n",
    "\n",
    "for i, button in enumerate(example_buttons):\n",
    "    button.on_click(create_example_handler(example_emails[i]))\n",
    "\n",
    "example_box = widgets.HBox(example_buttons)\n",
    "print(\"\\nOr try one of these examples:\")\n",
    "display(example_box)\n",
    "\n",
    "# Pre-click the analyze button to show initial result\n",
    "on_button_click(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate final report\n",
    "print(\"Generating final report...\")\n",
    "final_metrics = controller.evaluate(test_df)\n",
    "generate_report(controller, final_metrics)\n",
    "\n",
    "# Display the report\n",
    "with open('results/final_report.md', 'r') as f:\n",
    "    report_content = f.read()\n",
    "\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(report_content))\n",
    "\n",
    "# Show key metrics table\n",
    "print(\"\\n## Summary of Improvements\")\n",
    "\n",
    "# Create a results table\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'Clean Test Accuracy',\n",
    "        'Calibration ECE (Before)',\n",
    "        'Calibration ECE (After)',\n",
    "        'Average Attack Detection Rate (Baseline)',\n",
    "        'Average Attack Detection Rate (Robust)',\n",
    "        'Attack Detection Improvement'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{final_metrics['accuracy']:.4f}\",\n",
    "        f\"{calibration_results['original_ece']:.4f}\",\n",
    "        f\"{calibration_results['calibrated_ece']:.4f}\",\n",
    "        f\"{np.mean([m['baseline']['detection_rate'] for m in attack_metrics.values()]):.4f}\",\n",
    "        f\"{np.mean([m['robust']['detection_rate'] for m in attack_metrics.values()]):.4f}\",\n",
    "        f\"{np.mean([m['improvement'] for m in attack_metrics.values()]):.4f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
